{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36040796-22f1-4d86-96b2-91255e618951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def batch_encode_p1000(numbers):\n",
    "    \"\"\"\n",
    "    Encode a batch of numbers using the P1000 scheme.\n",
    "    \n",
    "    Parameters:\n",
    "      numbers (np.ndarray): Array of numbers to encode.\n",
    "      \n",
    "    Returns:\n",
    "      sign_tokens (np.ndarray): Array of sign tokens ('+' or '-').\n",
    "      mantissa_tokens (np.ndarray): Array of mantissa tokens as zero-padded strings.\n",
    "      exponent_tokens (np.ndarray): Array of exponent tokens as strings (e.g., 'E+3').\n",
    "    \"\"\"\n",
    "    # Convert input to numpy array\n",
    "    numbers = np.array(numbers, dtype=float)  # Ensure floating-point\n",
    "    \n",
    "    # Sign tokens: vectorized assignment based on the sign of the numbers\n",
    "    sign_tokens = np.where(numbers < 0, '-', '+')\n",
    "    \n",
    "    # Work with absolute values\n",
    "    abs_numbers = np.abs(numbers)\n",
    "    \n",
    "    # Handle zeros separately to avoid log10 issues\n",
    "    non_zero = abs_numbers > 0\n",
    "    exponent = np.zeros_like(abs_numbers, dtype=int)\n",
    "    normalized = np.zeros_like(abs_numbers, dtype=float)\n",
    "    \n",
    "    # Compute exponent where numbers are non-zero\n",
    "    exponent[non_zero] = np.floor(np.log10(abs_numbers[non_zero])).astype(int)\n",
    "    \n",
    "    # Compute normalized values for non-zero numbers - using float powers to avoid integer power error\n",
    "    normalized[non_zero] = abs_numbers[non_zero] / np.power(10.0, exponent[non_zero])\n",
    "    \n",
    "    # Scale the normalized values to get mantissa in [0, 1000)\n",
    "    # Multiply by 100 since normalized values are in [1, 10)\n",
    "    mantissas = np.round(normalized * 100).astype(int)\n",
    "    \n",
    "    # Correct any mantissa that rounds to 1000\n",
    "    overflow = mantissas >= 1000\n",
    "    if np.any(overflow):\n",
    "        mantissas[overflow] = 100\n",
    "        exponent[overflow] += 1\n",
    "    \n",
    "    # Format mantissas as 3-digit strings\n",
    "    mantissa_tokens = np.array([f\"{m:03d}\" for m in mantissas])\n",
    "    \n",
    "    # Format exponent tokens as strings (e.g., 'E+3' or 'E-2')\n",
    "    exponent_tokens = np.array([f\"E{'+' if e >= 0 else ''}{e}\" for e in exponent])\n",
    "    \n",
    "    return sign_tokens, mantissa_tokens, exponent_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84ee2f7-1cf8-4aa9-b52e-63b5b73a4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming batch_encode_p1000 function is already defined above\n",
    "\n",
    "def process_file_with_columns(file_path, col_sep_token=\"[COL_SEP]\",nrows=5,random_seed=42):\n",
    "    \"\"\"\n",
    "    Process a single CSV file containing numerical data.\n",
    "    Encodes each number using the P1000 scheme and preserves column structure by\n",
    "    inserting a delimiter token between columns.\n",
    "    \n",
    "    Returns a list of tokens for the entire file.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path,sep=r\"\\s+\",header=None)\n",
    "\n",
    "    if len(df)>nrows:\n",
    "        df=df.sample(n=nrows,random_state=random_seed)\n",
    "\n",
    "\n",
    "    \n",
    "    # We'll build a token list row by row\n",
    "    token_list = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        row_tokens = []\n",
    "        # Process each column in the row separately\n",
    "        for col_idx in df.columns:\n",
    "            # Encode the number in this cell; each cell may be a single number.\n",
    "            # If a cell contains multiple numbers, you might need to adjust this.\n",
    "            number = row[col_idx]\n",
    "            # For a single number, get the tokens as a list\n",
    "            sign_tokens, mantissa_tokens, exponent_tokens = batch_encode_p1000([number])\n",
    "            # Combine the triplet (they are arrays with one element each)\n",
    "            number_tokens = [sign_tokens[0], mantissa_tokens[0], exponent_tokens[0]]\n",
    "            row_tokens.extend(number_tokens)\n",
    "            # Insert column delimiter after each column (except last column)\n",
    "            if col_idx != df.columns[-1]:\n",
    "                row_tokens.append(col_sep_token)\n",
    "        # Optionally, you can add a row delimiter (e.g., [ROW_SEP]) if needed.\n",
    "        token_list.append(\"[ROW_SEP]\")\n",
    "        token_list.extend(row_tokens)\n",
    "    \n",
    "    token_list.append(\"[DATA_END]\")\n",
    "    return token_list\n",
    "\n",
    "# # Example usage for a single file:\n",
    "# file_path = \"../Feynman_with_units/I.6.2\"  # Update the path as needed\n",
    "# token_list = process_file_with_colum                                                                                                                   ns(file_path)\n",
    "# print(token_list[:20])  # Print first 20 tokens for inspection\n",
    "# print(\"Total tokens in file:\", len(token_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c74b78-cf90-451b-8bd1-74ce3b97c08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['II.8.31', 'III.12.43', 'I.18.4', 'II.10.9', 'II.11.27', 'III.21.20', 'I.18.14', 'III.9.52', 'I.6.2', 'II.36.38', 'II.2.42', 'II.13.34', 'I.13.12', 'III.4.33', 'I.12.5', 'II.34.29a', 'I.34.27', 'I.34.14', 'II.13.23', 'I.50.26', 'II.4.23', 'II.11.17', 'III.15.12', 'II.21.32', 'I.6.2b', 'II.11.20', 'I.39.11', 'II.6.15a', 'I.32.17', 'I.14.4', 'I.15.3x', 'II.27.18', 'I.6.2a', 'I.43.31', 'I.12.4', 'III.7.38', 'III.17.37', 'I.15.3t', 'III.10.19', 'II.6.11', 'I.32.5', 'I.39.1', 'I.34.8', 'I.13.4', 'II.34.2a', 'I.34.1', 'II.34.2', 'II.3.24', 'I.48.2', 'I.29.16', 'I.12.1', 'I.43.16', 'I.47.23', 'I.11.19', 'I.12.2', 'II.37.1', 'I.30.5', 'II.34.29b', 'I.40.1', 'II.11.3', 'III.13.18', 'I.39.22', 'I.15.1', 'II.24.17', 'I.26.2', 'I.24.6', 'II.38.3', 'III.15.27', 'I.25.13', 'III.14.14', 'II.27.16', 'III.15.14', 'II.35.18', 'I.14.3', 'I.37.4', 'I.38.12', 'III.4.32', 'III.8.54', 'I.18.12', 'II.15.4', 'I.12.11', 'I.44.4', 'I.41.16', 'II.15.5', 'II.35.21', 'II.38.14', 'I.9.18', 'I.43.43', 'I.16.6', 'II.6.15b', 'II.13.17', 'I.27.6', 'I.10.7', 'II.11.28', 'I.8.14', 'II.34.11', 'I.30.3', 'II.8.7', 'III.19.51', 'I.29.4']\n"
     ]
    }
   ],
   "source": [
    "folder = \"Feynman_with_units\"\n",
    "\n",
    "file_list = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e05161-6de9-440e-837d-1cd8a7e92635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c7f0aa-0f26-4f90-a8b8-ed2293982d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed II.8.31, token sequence length: 61\n",
      "Processed III.12.43, token sequence length: 61\n",
      "Processed I.18.4, token sequence length: 101\n",
      "Processed II.10.9, token sequence length: 81\n",
      "Processed II.11.27, token sequence length: 101\n",
      "Processed III.21.20, token sequence length: 101\n",
      "Processed I.18.14, token sequence length: 101\n",
      "Processed III.9.52, token sequence length: 141\n",
      "Processed I.6.2, token sequence length: 61\n",
      "Processed II.36.38, token sequence length: 181\n",
      "Processed II.2.42, token sequence length: 121\n",
      "Processed II.13.34, token sequence length: 81\n",
      "Processed I.13.12, token sequence length: 121\n",
      "Processed III.4.33, token sequence length: 101\n",
      "Processed I.12.5, token sequence length: 61\n",
      "Processed II.34.29a, token sequence length: 81\n",
      "Processed I.34.27, token sequence length: 61\n",
      "Processed I.34.14, token sequence length: 81\n",
      "Processed II.13.23, token sequence length: 81\n",
      "Processed I.50.26, token sequence length: 101\n",
      "Processed II.4.23, token sequence length: 81\n",
      "Processed II.11.17, token sequence length: 141\n",
      "Processed III.15.12, token sequence length: 81\n",
      "Processed II.21.32, token sequence length: 121\n",
      "Processed I.6.2b, token sequence length: 81\n",
      "Processed II.11.20, token sequence length: 121\n",
      "Processed I.39.11, token sequence length: 81\n",
      "Processed II.6.15a, token sequence length: 141\n",
      "Processed I.32.17, token sequence length: 141\n",
      "Processed I.14.4, token sequence length: 61\n",
      "Processed I.15.3x, token sequence length: 101\n",
      "Processed II.27.18, token sequence length: 61\n",
      "Processed I.6.2a, token sequence length: 41\n",
      "Processed I.43.31, token sequence length: 81\n",
      "Processed I.12.4, token sequence length: 81\n",
      "Processed III.7.38, token sequence length: 81\n",
      "Processed III.17.37, token sequence length: 81\n",
      "Processed I.15.3t, token sequence length: 101\n",
      "Processed III.10.19, token sequence length: 101\n",
      "Processed II.6.11, token sequence length: 101\n",
      "Processed I.32.5, token sequence length: 101\n",
      "Processed I.39.1, token sequence length: 61\n",
      "Processed I.34.8, token sequence length: 101\n",
      "Processed I.13.4, token sequence length: 101\n",
      "Processed II.34.2a, token sequence length: 81\n",
      "Processed I.34.1, token sequence length: 81\n",
      "Processed II.34.2, token sequence length: 81\n",
      "Processed II.3.24, token sequence length: 61\n",
      "Processed I.48.2, token sequence length: 81\n",
      "Processed I.29.16, token sequence length: 101\n",
      "Processed I.12.1, token sequence length: 61\n",
      "Processed I.43.16, token sequence length: 101\n",
      "Processed I.47.23, token sequence length: 81\n",
      "Processed I.11.19, token sequence length: 141\n",
      "Processed I.12.2, token sequence length: 101\n",
      "Processed II.37.1, token sequence length: 81\n",
      "Processed I.30.5, token sequence length: 81\n",
      "Processed II.34.29b, token sequence length: 121\n",
      "Processed I.40.1, token sequence length: 141\n",
      "Processed II.11.3, token sequence length: 121\n",
      "Processed III.13.18, token sequence length: 101\n",
      "Processed I.39.22, token sequence length: 101\n",
      "Processed I.15.1, token sequence length: 81\n",
      "Processed II.24.17, token sequence length: 81\n",
      "Processed I.26.2, token sequence length: 61\n",
      "Processed I.24.6, token sequence length: 101\n",
      "Processed II.38.3, token sequence length: 101\n",
      "Processed III.15.27, token sequence length: 81\n",
      "Processed I.25.13, token sequence length: 61\n",
      "Processed III.14.14, token sequence length: 121\n",
      "Processed II.27.16, token sequence length: 81\n",
      "Processed III.15.14, token sequence length: 81\n",
      "Processed II.35.18, token sequence length: 121\n",
      "Processed I.14.3, token sequence length: 81\n",
      "Processed I.37.4, token sequence length: 81\n",
      "Processed I.38.12, token sequence length: 101\n",
      "Processed III.4.32, token sequence length: 101\n",
      "Processed III.8.54, token sequence length: 81\n",
      "Processed I.18.12, token sequence length: 81\n",
      "Processed II.15.4, token sequence length: 81\n",
      "Processed I.12.11, token sequence length: 121\n",
      "Processed I.44.4, token sequence length: 121\n",
      "Processed I.41.16, token sequence length: 121\n",
      "Processed II.15.5, token sequence length: 81\n",
      "Processed II.35.21, token sequence length: 121\n",
      "Processed II.38.14, token sequence length: 61\n",
      "Processed I.9.18, token sequence length: 201\n",
      "Processed I.43.43, token sequence length: 101\n",
      "Processed I.16.6, token sequence length: 81\n",
      "Processed II.6.15b, token sequence length: 101\n",
      "Processed II.13.17, token sequence length: 101\n",
      "Processed I.27.6, token sequence length: 81\n",
      "Processed I.10.7, token sequence length: 81\n",
      "Processed II.11.28, token sequence length: 61\n",
      "Processed I.8.14, token sequence length: 101\n",
      "Processed II.34.11, token sequence length: 101\n",
      "Processed I.30.3, token sequence length: 81\n",
      "Processed II.8.7, token sequence length: 81\n",
      "Processed III.19.51, token sequence length: 121\n",
      "Processed I.29.4, token sequence length: 61\n"
     ]
    }
   ],
   "source": [
    "def process_all_files_with_columns(file_list, folder_path, col_sep_token=\"[COL_SEP]\"):\n",
    "    all_file_tokens = {}\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        tokens = process_file_with_columns(\n",
    "            file_path, \n",
    "            col_sep_token=col_sep_token, \n",
    "            nrows=5,          # <--- ensures we only get 50 rows\n",
    "            random_seed=42\n",
    "        )\n",
    "        all_file_tokens[file_name] = tokens\n",
    "        print(f\"Processed {file_name}, token sequence length: {len(tokens)}\")\n",
    "    return all_file_tokens\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"Feynman_with_units\"\n",
    "all_tokens = process_all_files_with_columns(file_list,folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c9a65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa47ef98-f22e-4ae5-8ade-731e16109c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer=Tokenizer.from_file(\"my_tokenizer.json\")\n",
    "tokenized_dict={}\n",
    "for key,value in all_tokens.items():\n",
    "    current_token_string=\" \".join(value)\n",
    "    encoding_result = tokenizer.encode(current_token_string)  \n",
    "    tokenized_dict[key] = encoding_result.ids\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2518f863-cacc-4b48-93c7-872a8c1bbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"dataset_encoded.json\", \"w\") as f:\n",
    "    json.dump(tokenized_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe0350fe-606c-463f-a59f-ba7da820443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formulas saved to filenames.txt\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "wb = load_workbook('FeynmanEquations.xlsx', data_only=False)\n",
    "ws = wb.active\n",
    "\n",
    "# Iterate through rows (we start from row 2 because we skip header)\n",
    "formula_col_index = 1 \n",
    "formula_list=[]\n",
    "# Iterate through rows (we start from row 2 because we skip header)\n",
    "for row in ws.iter_rows(min_row=2):\n",
    "    # get cell in formula column\n",
    "    cell = row[formula_col_index - 1]  # zero-indexed\n",
    "    formula_list.append(cell.value) \n",
    "    \n",
    "\n",
    "##Save the formula_list in a text file\n",
    "filename = \"filenames.txt\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for formula in formula_list:\n",
    "        f.write(formula + \"\\n\")\n",
    "\n",
    "print(f\"Formulas saved to {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e1d3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATA_END_ID = 4  # replace with the actual token ID for [DATA_END]\n",
    "combined_samples = []\n",
    "\n",
    "# 1) Load RPN file\n",
    "with open(\"formulas_rpn.json\", \"r\") as f:\n",
    "    rpn_data = json.load(f)  # a list of dicts: [{\"id\": \"I.6.2a\", \"rpn\": [...]}, ...]\n",
    "\n",
    "# 2) Load numeric-data file\n",
    "with open(\"dataset_encoded.json\", \"r\") as f:\n",
    "    data_dict = json.load(f) # a dict with keys like \"I.6.2a\" : [ data tokens ], etc.\n",
    "\n",
    "# 3) Combine data & RPN for each entry\n",
    "for item in rpn_data:\n",
    "    formula_id = item[\"id\"]\n",
    "    rpn_tokens = item[\"rpn\"]\n",
    "\n",
    "    # Find matching data tokens (same ID) in data_dict\n",
    "    if formula_id in data_dict:\n",
    "        data_tokens = data_dict[formula_id]\n",
    "        combined = data_tokens + [DATA_END_ID] + rpn_tokens\n",
    "        combined_samples.append(combined)\n",
    "    else:\n",
    "        print(f\"Warning: {formula_id} not found in dataset_encoded.json\")\n",
    "\n",
    "# 'combined_samples' now contains your full sequences, each with data followed by [DATA_END] and formula RPN tokens.\n",
    "with open(\"combined_samples.json\", \"w\") as f:\n",
    "    json.dump(combined_samples, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979788fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Returns the Levenshtein edit distance between two lists of tokens,\n",
    "    i.e. the minimum number of edits (insertions, deletions, substitutions)\n",
    "    required to transform seq1 into seq2.\n",
    "    \"\"\"\n",
    "    # seq1, seq2 are lists of token IDs (or anything hashable).\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "    \n",
    "    # Create a DP table (len1+1) x (len2+1)\n",
    "    dp = [[0] * (len2 + 1) for _ in range(len1 + 1)]\n",
    "    \n",
    "    # Initialization\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Compute DP\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            if seq1[i-1] == seq2[j-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i-1][j] + 1,      # deletion\n",
    "                dp[i][j-1] + 1,      # insertion\n",
    "                dp[i-1][j-1] + cost  # substitution\n",
    "            )\n",
    "    return dp[len1][len2]\n",
    "\n",
    "\n",
    "def normalized_edit_distance(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Returns the normalized edit distance between two sequences in [0..1].\n",
    "    0 = identical sequences, 1 = completely different if length is > 0.\n",
    "    \"\"\"\n",
    "    if not seq1 and not seq2:\n",
    "        return 0.0  # both empty\n",
    "    dist = levenshtein_distance(seq1, seq2)\n",
    "    max_len = max(len(seq1), len(seq2))\n",
    "    return dist / max_len\n",
    "\n",
    "\n",
    "def edit_distance_score(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Returns a similarity score in [0..1], \n",
    "    where 1.0 = identical sequences, 0.0 = completely different.\n",
    "    \"\"\"\n",
    "    ned = normalized_edit_distance(seq1, seq2)\n",
    "    return 1.0 - ned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518a3117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 100\n",
      "Average sequence length: 94.01\n",
      "Max sequence length: 201\n",
      "Average Length:94.01\n",
      "Estimated fraction fully correct at 70% token acc: 0.000000\n",
      "I want the code to be executed only till here.\n",
      "Sample (raw) sequences from all_sequences:\n",
      "Example 0: [3, 647, 1169, 1731, 2, 647, 1176, 1731, 2, 647, 1011, 1731, 2, 647, 1142, 1731, 2, 648, 1246, 1732, 3, 647, 935, 1731, 2, 647, 946, 1731, 2, 647, 966, 1731, 2, 647, 1168, 1731, 2, 648, 837, 1732, 3, 647, 1119, 1731, 2, 647, 877, 1731, 2, 647, 885, 1731, 2, 647, 1068, 1731, 2, 648, 1065, 1731, 3, 647, 1089, 1731]\n",
      "Length: 64\n",
      "--------\n",
      "Example 1: [3, 647, 930, 1731, 2, 647, 1140, 1731, 2, 647, 1031, 1731, 2, 647, 990, 1731, 2, 647, 1227, 1731, 3, 647, 1032, 1731, 2, 647, 1106, 1731, 2, 647, 902, 1731, 2, 647, 1062, 1731, 2, 647, 1040, 1731, 3, 647, 948, 1731, 2, 647, 1102, 1731, 2, 647, 1132, 1731, 2, 647, 874, 1731, 2, 647, 836, 1732, 3, 647, 1189, 1731]\n",
      "Length: 64\n",
      "--------\n",
      "Example 2: [2, 647, 1021, 1731, 2, 647, 1203, 1731, 2, 647, 824, 1731, 2, 647, 1184, 1732, 3, 647, 1176, 1731, 2, 647, 973, 1731, 2, 647, 1002, 1731, 2, 647, 1038, 1731, 2, 648, 853, 1731, 4]\n",
      "Length: 37\n",
      "--------\n",
      "Number of short test sequences (<= 8 tokens): 0\n",
      "No short sequences to test.\n",
      "Sample input shape: torch.Size([154, 63])\n",
      "Sample target shape: torch.Size([154, 63])\n",
      "Sample mask shape: torch.Size([154, 63])\n",
      "Sample input: tensor([   3,  647,  860, 1731,    2,  647,  933, 1731,    2,  647])\n",
      "Sample target: tensor([ 647,  860, 1731,    2,  647,  933, 1731,    2,  647,  913])\n",
      "Max token ID in input: 1734\n",
      "Max token ID in target: 1734\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 1/500] Train Loss: 7.8214 | Train Acc: 0.0000 || Val Loss: 5.2580 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 2/500] Train Loss: 5.2719 | Train Acc: 0.2609 || Val Loss: 4.1481 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 3/500] Train Loss: 4.1078 | Train Acc: 0.2475 || Val Loss: 3.9260 | Val Acc: 0.1924\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 4/500] Train Loss: 3.8548 | Train Acc: 0.2060 || Val Loss: 3.8429 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 5/500] Train Loss: 3.7490 | Train Acc: 0.2476 || Val Loss: 3.7528 | Val Acc: 0.2114\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 6/500] Train Loss: 3.6572 | Train Acc: 0.2277 || Val Loss: 3.6678 | Val Acc: 0.2114\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 7/500] Train Loss: 3.5749 | Train Acc: 0.2221 || Val Loss: 3.5969 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 8/500] Train Loss: 3.5099 | Train Acc: 0.2442 || Val Loss: 3.5571 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 9/500] Train Loss: 3.4724 | Train Acc: 0.2468 || Val Loss: 3.5239 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 10/500] Train Loss: 3.4352 | Train Acc: 0.2429 || Val Loss: 3.4898 | Val Acc: 0.2114\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 11/500] Train Loss: 3.3926 | Train Acc: 0.2288 || Val Loss: 3.4619 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 12/500] Train Loss: 3.3531 | Train Acc: 0.2357 || Val Loss: 3.4494 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 13/500] Train Loss: 3.3323 | Train Acc: 0.2421 || Val Loss: 3.4452 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 14/500] Train Loss: 3.3199 | Train Acc: 0.2460 || Val Loss: 3.4407 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 15/500] Train Loss: 3.3120 | Train Acc: 0.2439 || Val Loss: 3.4374 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 16/500] Train Loss: 3.3023 | Train Acc: 0.2322 || Val Loss: 3.4331 | Val Acc: 0.2114\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 17/500] Train Loss: 3.2953 | Train Acc: 0.2273 || Val Loss: 3.4276 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 18/500] Train Loss: 3.2860 | Train Acc: 0.2338 || Val Loss: 3.4246 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 19/500] Train Loss: 3.2805 | Train Acc: 0.2420 || Val Loss: 3.4233 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 20/500] Train Loss: 3.2736 | Train Acc: 0.2442 || Val Loss: 3.4230 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 21/500] Train Loss: 3.2728 | Train Acc: 0.2388 || Val Loss: 3.4244 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 22/500] Train Loss: 3.2666 | Train Acc: 0.2418 || Val Loss: 3.4267 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 23/500] Train Loss: 3.2647 | Train Acc: 0.2345 || Val Loss: 3.4281 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 24/500] Train Loss: 3.2650 | Train Acc: 0.2310 || Val Loss: 3.4297 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 25/500] Train Loss: 3.2595 | Train Acc: 0.2398 || Val Loss: 3.4317 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 26/500] Train Loss: 3.2578 | Train Acc: 0.2391 || Val Loss: 3.4325 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 27/500] Train Loss: 3.2566 | Train Acc: 0.2479 || Val Loss: 3.4318 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 28/500] Train Loss: 3.2586 | Train Acc: 0.2393 || Val Loss: 3.4302 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 29/500] Train Loss: 3.2558 | Train Acc: 0.2416 || Val Loss: 3.4271 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 30/500] Train Loss: 3.2538 | Train Acc: 0.2335 || Val Loss: 3.4235 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 31/500] Train Loss: 3.2512 | Train Acc: 0.2286 || Val Loss: 3.4209 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 32/500] Train Loss: 3.2524 | Train Acc: 0.2351 || Val Loss: 3.4195 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 33/500] Train Loss: 3.2512 | Train Acc: 0.2439 || Val Loss: 3.4189 | Val Acc: 0.2442\n",
      "  (Best validation loss so far, saving model)\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 34/500] Train Loss: 3.2485 | Train Acc: 0.2434 || Val Loss: 3.4196 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 35/500] Train Loss: 3.2498 | Train Acc: 0.2308 || Val Loss: 3.4211 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 36/500] Train Loss: 3.2486 | Train Acc: 0.2375 || Val Loss: 3.4229 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 37/500] Train Loss: 3.2478 | Train Acc: 0.2359 || Val Loss: 3.4253 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 38/500] Train Loss: 3.2471 | Train Acc: 0.2384 || Val Loss: 3.4282 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 39/500] Train Loss: 3.2473 | Train Acc: 0.2439 || Val Loss: 3.4310 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 40/500] Train Loss: 3.2465 | Train Acc: 0.2443 || Val Loss: 3.4333 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 41/500] Train Loss: 3.2473 | Train Acc: 0.2342 || Val Loss: 3.4351 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 42/500] Train Loss: 3.2448 | Train Acc: 0.2434 || Val Loss: 3.4360 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 43/500] Train Loss: 3.2461 | Train Acc: 0.2429 || Val Loss: 3.4364 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 44/500] Train Loss: 3.2445 | Train Acc: 0.2450 || Val Loss: 3.4366 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 45/500] Train Loss: 3.2454 | Train Acc: 0.2445 || Val Loss: 3.4366 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 46/500] Train Loss: 3.2452 | Train Acc: 0.2428 || Val Loss: 3.4365 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 47/500] Train Loss: 3.2442 | Train Acc: 0.2399 || Val Loss: 3.4366 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 48/500] Train Loss: 3.2466 | Train Acc: 0.2375 || Val Loss: 3.4366 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 49/500] Train Loss: 3.2438 | Train Acc: 0.2413 || Val Loss: 3.4365 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 50/500] Train Loss: 3.2436 | Train Acc: 0.2399 || Val Loss: 3.4366 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 51/500] Train Loss: 3.2441 | Train Acc: 0.2416 || Val Loss: 3.4367 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 52/500] Train Loss: 3.2439 | Train Acc: 0.2402 || Val Loss: 3.4368 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 53/500] Train Loss: 3.2439 | Train Acc: 0.2366 || Val Loss: 3.4370 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 54/500] Train Loss: 3.2430 | Train Acc: 0.2359 || Val Loss: 3.4370 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 55/500] Train Loss: 3.2458 | Train Acc: 0.2389 || Val Loss: 3.4371 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 56/500] Train Loss: 3.2437 | Train Acc: 0.2438 || Val Loss: 3.4374 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 57/500] Train Loss: 3.2433 | Train Acc: 0.2407 || Val Loss: 3.4380 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 58/500] Train Loss: 3.2444 | Train Acc: 0.2481 || Val Loss: 3.4389 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 59/500] Train Loss: 3.2441 | Train Acc: 0.2353 || Val Loss: 3.4398 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 60/500] Train Loss: 3.2424 | Train Acc: 0.2454 || Val Loss: 3.4406 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 61/500] Train Loss: 3.2438 | Train Acc: 0.2363 || Val Loss: 3.4411 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 62/500] Train Loss: 3.2441 | Train Acc: 0.2348 || Val Loss: 3.4414 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 63/500] Train Loss: 3.2437 | Train Acc: 0.2393 || Val Loss: 3.4415 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 64/500] Train Loss: 3.2424 | Train Acc: 0.2425 || Val Loss: 3.4414 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 65/500] Train Loss: 3.2441 | Train Acc: 0.2371 || Val Loss: 3.4412 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 66/500] Train Loss: 3.2429 | Train Acc: 0.2342 || Val Loss: 3.4408 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 67/500] Train Loss: 3.2428 | Train Acc: 0.2424 || Val Loss: 3.4405 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 68/500] Train Loss: 3.2438 | Train Acc: 0.2413 || Val Loss: 3.4403 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 69/500] Train Loss: 3.2413 | Train Acc: 0.2380 || Val Loss: 3.4404 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 70/500] Train Loss: 3.2424 | Train Acc: 0.2434 || Val Loss: 3.4407 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 71/500] Train Loss: 3.2420 | Train Acc: 0.2411 || Val Loss: 3.4411 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 72/500] Train Loss: 3.2433 | Train Acc: 0.2414 || Val Loss: 3.4416 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 73/500] Train Loss: 3.2447 | Train Acc: 0.2337 || Val Loss: 3.4421 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 74/500] Train Loss: 3.2431 | Train Acc: 0.2355 || Val Loss: 3.4426 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 75/500] Train Loss: 3.2424 | Train Acc: 0.2392 || Val Loss: 3.4429 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 76/500] Train Loss: 3.2420 | Train Acc: 0.2434 || Val Loss: 3.4431 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 77/500] Train Loss: 3.2412 | Train Acc: 0.2413 || Val Loss: 3.4432 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 78/500] Train Loss: 3.2426 | Train Acc: 0.2406 || Val Loss: 3.4433 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 79/500] Train Loss: 3.2405 | Train Acc: 0.2385 || Val Loss: 3.4435 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 80/500] Train Loss: 3.2417 | Train Acc: 0.2399 || Val Loss: 3.4437 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 81/500] Train Loss: 3.2436 | Train Acc: 0.2406 || Val Loss: 3.4439 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 82/500] Train Loss: 3.2412 | Train Acc: 0.2423 || Val Loss: 3.4441 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 83/500] Train Loss: 3.2424 | Train Acc: 0.2400 || Val Loss: 3.4442 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 84/500] Train Loss: 3.2414 | Train Acc: 0.2357 || Val Loss: 3.4442 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 85/500] Train Loss: 3.2422 | Train Acc: 0.2436 || Val Loss: 3.4442 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 86/500] Train Loss: 3.2417 | Train Acc: 0.2476 || Val Loss: 3.4442 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 87/500] Train Loss: 3.2418 | Train Acc: 0.2460 || Val Loss: 3.4442 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 88/500] Train Loss: 3.2416 | Train Acc: 0.2407 || Val Loss: 3.4442 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 89/500] Train Loss: 3.2414 | Train Acc: 0.2367 || Val Loss: 3.4444 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 90/500] Train Loss: 3.2420 | Train Acc: 0.2418 || Val Loss: 3.4446 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 91/500] Train Loss: 3.2424 | Train Acc: 0.2447 || Val Loss: 3.4447 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 92/500] Train Loss: 3.2405 | Train Acc: 0.2439 || Val Loss: 3.4448 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 93/500] Train Loss: 3.2402 | Train Acc: 0.2446 || Val Loss: 3.4450 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 94/500] Train Loss: 3.2428 | Train Acc: 0.2356 || Val Loss: 3.4451 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 95/500] Train Loss: 3.2427 | Train Acc: 0.2423 || Val Loss: 3.4453 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 96/500] Train Loss: 3.2411 | Train Acc: 0.2428 || Val Loss: 3.4455 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 97/500] Train Loss: 3.2424 | Train Acc: 0.2380 || Val Loss: 3.4456 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 98/500] Train Loss: 3.2418 | Train Acc: 0.2395 || Val Loss: 3.4457 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 99/500] Train Loss: 3.2388 | Train Acc: 0.2438 || Val Loss: 3.4459 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 100/500] Train Loss: 3.2412 | Train Acc: 0.2400 || Val Loss: 3.4462 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 101/500] Train Loss: 3.2412 | Train Acc: 0.2427 || Val Loss: 3.4464 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 102/500] Train Loss: 3.2393 | Train Acc: 0.2441 || Val Loss: 3.4466 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 103/500] Train Loss: 3.2396 | Train Acc: 0.2471 || Val Loss: 3.4468 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 104/500] Train Loss: 3.2407 | Train Acc: 0.2424 || Val Loss: 3.4469 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 105/500] Train Loss: 3.2391 | Train Acc: 0.2428 || Val Loss: 3.4469 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 106/500] Train Loss: 3.2424 | Train Acc: 0.2424 || Val Loss: 3.4470 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 107/500] Train Loss: 3.2419 | Train Acc: 0.2453 || Val Loss: 3.4470 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 108/500] Train Loss: 3.2408 | Train Acc: 0.2429 || Val Loss: 3.4472 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 109/500] Train Loss: 3.2398 | Train Acc: 0.2482 || Val Loss: 3.4474 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 110/500] Train Loss: 3.2395 | Train Acc: 0.2468 || Val Loss: 3.4476 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 111/500] Train Loss: 3.2411 | Train Acc: 0.2456 || Val Loss: 3.4477 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 112/500] Train Loss: 3.2411 | Train Acc: 0.2457 || Val Loss: 3.4478 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 113/500] Train Loss: 3.2404 | Train Acc: 0.2411 || Val Loss: 3.4481 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 114/500] Train Loss: 3.2410 | Train Acc: 0.2409 || Val Loss: 3.4483 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 115/500] Train Loss: 3.2404 | Train Acc: 0.2441 || Val Loss: 3.4483 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 116/500] Train Loss: 3.2417 | Train Acc: 0.2427 || Val Loss: 3.4482 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 117/500] Train Loss: 3.2409 | Train Acc: 0.2367 || Val Loss: 3.4482 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 118/500] Train Loss: 3.2397 | Train Acc: 0.2392 || Val Loss: 3.4482 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 119/500] Train Loss: 3.2415 | Train Acc: 0.2384 || Val Loss: 3.4481 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 120/500] Train Loss: 3.2415 | Train Acc: 0.2413 || Val Loss: 3.4482 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 121/500] Train Loss: 3.2416 | Train Acc: 0.2360 || Val Loss: 3.4483 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 122/500] Train Loss: 3.2394 | Train Acc: 0.2493 || Val Loss: 3.4484 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 123/500] Train Loss: 3.2380 | Train Acc: 0.2400 || Val Loss: 3.4487 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 124/500] Train Loss: 3.2404 | Train Acc: 0.2400 || Val Loss: 3.4491 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 125/500] Train Loss: 3.2401 | Train Acc: 0.2428 || Val Loss: 3.4495 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 126/500] Train Loss: 3.2399 | Train Acc: 0.2441 || Val Loss: 3.4498 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 127/500] Train Loss: 3.2406 | Train Acc: 0.2381 || Val Loss: 3.4500 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 128/500] Train Loss: 3.2408 | Train Acc: 0.2421 || Val Loss: 3.4501 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 129/500] Train Loss: 3.2391 | Train Acc: 0.2416 || Val Loss: 3.4499 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 130/500] Train Loss: 3.2396 | Train Acc: 0.2393 || Val Loss: 3.4498 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 131/500] Train Loss: 3.2404 | Train Acc: 0.2382 || Val Loss: 3.4499 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 132/500] Train Loss: 3.2398 | Train Acc: 0.2398 || Val Loss: 3.4500 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 133/500] Train Loss: 3.2386 | Train Acc: 0.2423 || Val Loss: 3.4502 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 134/500] Train Loss: 3.2395 | Train Acc: 0.2398 || Val Loss: 3.4505 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 135/500] Train Loss: 3.2387 | Train Acc: 0.2432 || Val Loss: 3.4503 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 136/500] Train Loss: 3.2393 | Train Acc: 0.2475 || Val Loss: 3.4503 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 137/500] Train Loss: 3.2409 | Train Acc: 0.2417 || Val Loss: 3.4505 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 138/500] Train Loss: 3.2385 | Train Acc: 0.2417 || Val Loss: 3.4507 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 139/500] Train Loss: 3.2402 | Train Acc: 0.2474 || Val Loss: 3.4509 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 140/500] Train Loss: 3.2417 | Train Acc: 0.2385 || Val Loss: 3.4512 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 141/500] Train Loss: 3.2390 | Train Acc: 0.2405 || Val Loss: 3.4514 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 142/500] Train Loss: 3.2393 | Train Acc: 0.2420 || Val Loss: 3.4514 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 143/500] Train Loss: 3.2385 | Train Acc: 0.2411 || Val Loss: 3.4514 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 144/500] Train Loss: 3.2388 | Train Acc: 0.2494 || Val Loss: 3.4514 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 145/500] Train Loss: 3.2390 | Train Acc: 0.2402 || Val Loss: 3.4514 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 146/500] Train Loss: 3.2394 | Train Acc: 0.2429 || Val Loss: 3.4516 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 147/500] Train Loss: 3.2408 | Train Acc: 0.2399 || Val Loss: 3.4517 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 148/500] Train Loss: 3.2388 | Train Acc: 0.2438 || Val Loss: 3.4519 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 149/500] Train Loss: 3.2391 | Train Acc: 0.2434 || Val Loss: 3.4521 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 150/500] Train Loss: 3.2386 | Train Acc: 0.2424 || Val Loss: 3.4523 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 151/500] Train Loss: 3.2409 | Train Acc: 0.2403 || Val Loss: 3.4523 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 152/500] Train Loss: 3.2394 | Train Acc: 0.2409 || Val Loss: 3.4522 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 153/500] Train Loss: 3.2391 | Train Acc: 0.2428 || Val Loss: 3.4523 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 154/500] Train Loss: 3.2398 | Train Acc: 0.2398 || Val Loss: 3.4530 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 155/500] Train Loss: 3.2384 | Train Acc: 0.2478 || Val Loss: 3.4529 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 156/500] Train Loss: 3.2389 | Train Acc: 0.2443 || Val Loss: 3.4530 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 157/500] Train Loss: 3.2368 | Train Acc: 0.2488 || Val Loss: 3.4536 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 158/500] Train Loss: 3.2396 | Train Acc: 0.2407 || Val Loss: 3.4532 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 159/500] Train Loss: 3.2392 | Train Acc: 0.2384 || Val Loss: 3.4533 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 160/500] Train Loss: 3.2379 | Train Acc: 0.2421 || Val Loss: 3.4540 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 161/500] Train Loss: 3.2376 | Train Acc: 0.2485 || Val Loss: 3.4537 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 162/500] Train Loss: 3.2374 | Train Acc: 0.2461 || Val Loss: 3.4538 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 163/500] Train Loss: 3.2396 | Train Acc: 0.2447 || Val Loss: 3.4542 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 164/500] Train Loss: 3.2392 | Train Acc: 0.2421 || Val Loss: 3.4542 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 165/500] Train Loss: 3.2390 | Train Acc: 0.2405 || Val Loss: 3.4537 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 166/500] Train Loss: 3.2392 | Train Acc: 0.2425 || Val Loss: 3.4541 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 167/500] Train Loss: 3.2395 | Train Acc: 0.2456 || Val Loss: 3.4543 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 168/500] Train Loss: 3.2383 | Train Acc: 0.2400 || Val Loss: 3.4541 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 169/500] Train Loss: 3.2380 | Train Acc: 0.2424 || Val Loss: 3.4558 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 170/500] Train Loss: 3.2397 | Train Acc: 0.2482 || Val Loss: 3.4549 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 171/500] Train Loss: 3.2382 | Train Acc: 0.2424 || Val Loss: 3.4551 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 172/500] Train Loss: 3.2384 | Train Acc: 0.2457 || Val Loss: 3.4561 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 173/500] Train Loss: 3.2401 | Train Acc: 0.2464 || Val Loss: 3.4553 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 174/500] Train Loss: 3.2408 | Train Acc: 0.2420 || Val Loss: 3.4559 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 175/500] Train Loss: 3.2379 | Train Acc: 0.2476 || Val Loss: 3.4570 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 176/500] Train Loss: 3.2369 | Train Acc: 0.2457 || Val Loss: 3.4561 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 177/500] Train Loss: 3.2379 | Train Acc: 0.2479 || Val Loss: 3.4568 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 178/500] Train Loss: 3.2396 | Train Acc: 0.2454 || Val Loss: 3.4569 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 179/500] Train Loss: 3.2390 | Train Acc: 0.2427 || Val Loss: 3.4556 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 180/500] Train Loss: 3.2385 | Train Acc: 0.2442 || Val Loss: 3.4568 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 181/500] Train Loss: 3.2393 | Train Acc: 0.2414 || Val Loss: 3.4567 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 182/500] Train Loss: 3.2370 | Train Acc: 0.2421 || Val Loss: 3.4559 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 183/500] Train Loss: 3.2379 | Train Acc: 0.2370 || Val Loss: 3.4573 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 184/500] Train Loss: 3.2377 | Train Acc: 0.2416 || Val Loss: 3.4568 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 185/500] Train Loss: 3.2371 | Train Acc: 0.2468 || Val Loss: 3.4570 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 186/500] Train Loss: 3.2397 | Train Acc: 0.2409 || Val Loss: 3.4568 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 187/500] Train Loss: 3.2390 | Train Acc: 0.2438 || Val Loss: 3.4583 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 188/500] Train Loss: 3.2382 | Train Acc: 0.2461 || Val Loss: 3.4583 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 189/500] Train Loss: 3.2388 | Train Acc: 0.2428 || Val Loss: 3.4573 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 190/500] Train Loss: 3.2363 | Train Acc: 0.2514 || Val Loss: 3.4595 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 191/500] Train Loss: 3.2388 | Train Acc: 0.2464 || Val Loss: 3.4570 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 192/500] Train Loss: 3.2400 | Train Acc: 0.2380 || Val Loss: 3.4592 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 193/500] Train Loss: 3.2379 | Train Acc: 0.2454 || Val Loss: 3.4576 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 194/500] Train Loss: 3.2378 | Train Acc: 0.2432 || Val Loss: 3.4597 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 195/500] Train Loss: 3.2375 | Train Acc: 0.2453 || Val Loss: 3.4582 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 196/500] Train Loss: 3.2367 | Train Acc: 0.2449 || Val Loss: 3.4591 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 197/500] Train Loss: 3.2363 | Train Acc: 0.2478 || Val Loss: 3.4611 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 198/500] Train Loss: 3.2365 | Train Acc: 0.2453 || Val Loss: 3.4570 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 199/500] Train Loss: 3.2386 | Train Acc: 0.2340 || Val Loss: 3.4669 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 200/500] Train Loss: 3.2411 | Train Acc: 0.2467 || Val Loss: 3.4577 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 201/500] Train Loss: 3.2418 | Train Acc: 0.2244 || Val Loss: 3.4690 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 202/500] Train Loss: 3.2399 | Train Acc: 0.2482 || Val Loss: 3.4590 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 203/500] Train Loss: 3.2379 | Train Acc: 0.2409 || Val Loss: 3.4581 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 204/500] Train Loss: 3.2390 | Train Acc: 0.2322 || Val Loss: 3.4690 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 205/500] Train Loss: 3.2402 | Train Acc: 0.2468 || Val Loss: 3.4584 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 206/500] Train Loss: 3.2420 | Train Acc: 0.2232 || Val Loss: 3.4617 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 207/500] Train Loss: 3.2385 | Train Acc: 0.2470 || Val Loss: 3.4655 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 208/500] Train Loss: 3.2375 | Train Acc: 0.2483 || Val Loss: 3.4596 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 209/500] Train Loss: 3.2400 | Train Acc: 0.2362 || Val Loss: 3.4620 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 210/500] Train Loss: 3.2374 | Train Acc: 0.2463 || Val Loss: 3.4631 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 211/500] Train Loss: 3.2401 | Train Acc: 0.2435 || Val Loss: 3.4592 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 212/500] Train Loss: 3.2386 | Train Acc: 0.2344 || Val Loss: 3.4631 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 213/500] Train Loss: 3.2364 | Train Acc: 0.2436 || Val Loss: 3.4636 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 214/500] Train Loss: 3.2365 | Train Acc: 0.2454 || Val Loss: 3.4596 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 215/500] Train Loss: 3.2384 | Train Acc: 0.2411 || Val Loss: 3.4633 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 216/500] Train Loss: 3.2363 | Train Acc: 0.2445 || Val Loss: 3.4651 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 217/500] Train Loss: 3.2369 | Train Acc: 0.2450 || Val Loss: 3.4603 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 218/500] Train Loss: 3.2377 | Train Acc: 0.2374 || Val Loss: 3.4629 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 219/500] Train Loss: 3.2368 | Train Acc: 0.2411 || Val Loss: 3.4639 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 220/500] Train Loss: 3.2375 | Train Acc: 0.2428 || Val Loss: 3.4623 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 221/500] Train Loss: 3.2371 | Train Acc: 0.2486 || Val Loss: 3.4639 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 222/500] Train Loss: 3.2373 | Train Acc: 0.2463 || Val Loss: 3.4630 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 223/500] Train Loss: 3.2369 | Train Acc: 0.2449 || Val Loss: 3.4657 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 224/500] Train Loss: 3.2359 | Train Acc: 0.2461 || Val Loss: 3.4641 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 225/500] Train Loss: 3.2351 | Train Acc: 0.2439 || Val Loss: 3.4647 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 226/500] Train Loss: 3.2371 | Train Acc: 0.2453 || Val Loss: 3.4662 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 227/500] Train Loss: 3.2359 | Train Acc: 0.2428 || Val Loss: 3.4648 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 228/500] Train Loss: 3.2391 | Train Acc: 0.2370 || Val Loss: 3.4675 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 229/500] Train Loss: 3.2363 | Train Acc: 0.2409 || Val Loss: 3.4674 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 230/500] Train Loss: 3.2368 | Train Acc: 0.2435 || Val Loss: 3.4668 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 231/500] Train Loss: 3.2367 | Train Acc: 0.2425 || Val Loss: 3.4697 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 232/500] Train Loss: 3.2362 | Train Acc: 0.2423 || Val Loss: 3.4670 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 233/500] Train Loss: 3.2369 | Train Acc: 0.2427 || Val Loss: 3.4709 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 234/500] Train Loss: 3.2367 | Train Acc: 0.2424 || Val Loss: 3.4708 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 235/500] Train Loss: 3.2336 | Train Acc: 0.2438 || Val Loss: 3.4705 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 236/500] Train Loss: 3.2359 | Train Acc: 0.2434 || Val Loss: 3.4736 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 237/500] Train Loss: 3.2368 | Train Acc: 0.2406 || Val Loss: 3.4616 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 238/500] Train Loss: 3.2387 | Train Acc: 0.2337 || Val Loss: 3.5160 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 239/500] Train Loss: 3.2538 | Train Acc: 0.2475 || Val Loss: 3.4703 | Val Acc: 0.2114\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 240/500] Train Loss: 3.2715 | Train Acc: 0.2124 || Val Loss: 3.4987 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 241/500] Train Loss: 3.2431 | Train Acc: 0.2475 || Val Loss: 3.5059 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 242/500] Train Loss: 3.2491 | Train Acc: 0.2475 || Val Loss: 3.4710 | Val Acc: 0.2114\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 243/500] Train Loss: 3.2611 | Train Acc: 0.2196 || Val Loss: 3.4648 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 244/500] Train Loss: 3.2390 | Train Acc: 0.2385 || Val Loss: 3.5153 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 245/500] Train Loss: 3.2563 | Train Acc: 0.2475 || Val Loss: 3.4827 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 246/500] Train Loss: 3.2391 | Train Acc: 0.2486 || Val Loss: 3.4673 | Val Acc: 0.2114\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 247/500] Train Loss: 3.2469 | Train Acc: 0.2232 || Val Loss: 3.4657 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 248/500] Train Loss: 3.2455 | Train Acc: 0.2179 || Val Loss: 3.4765 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 249/500] Train Loss: 3.2368 | Train Acc: 0.2478 || Val Loss: 3.4918 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 250/500] Train Loss: 3.2490 | Train Acc: 0.2475 || Val Loss: 3.4679 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 251/500] Train Loss: 3.2380 | Train Acc: 0.2439 || Val Loss: 3.4661 | Val Acc: 0.2114\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 252/500] Train Loss: 3.2441 | Train Acc: 0.2216 || Val Loss: 3.4643 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 253/500] Train Loss: 3.2403 | Train Acc: 0.2205 || Val Loss: 3.4730 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 254/500] Train Loss: 3.2368 | Train Acc: 0.2483 || Val Loss: 3.4860 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 255/500] Train Loss: 3.2435 | Train Acc: 0.2474 || Val Loss: 3.4729 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 256/500] Train Loss: 3.2367 | Train Acc: 0.2457 || Val Loss: 3.4684 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 257/500] Train Loss: 3.2410 | Train Acc: 0.2270 || Val Loss: 3.4684 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 258/500] Train Loss: 3.2398 | Train Acc: 0.2233 || Val Loss: 3.4715 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 259/500] Train Loss: 3.2359 | Train Acc: 0.2436 || Val Loss: 3.4818 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 260/500] Train Loss: 3.2394 | Train Acc: 0.2478 || Val Loss: 3.4764 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 261/500] Train Loss: 3.2391 | Train Acc: 0.2482 || Val Loss: 3.4679 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 262/500] Train Loss: 3.2365 | Train Acc: 0.2373 || Val Loss: 3.4680 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 263/500] Train Loss: 3.2395 | Train Acc: 0.2295 || Val Loss: 3.4692 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 264/500] Train Loss: 3.2361 | Train Acc: 0.2418 || Val Loss: 3.4758 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 265/500] Train Loss: 3.2367 | Train Acc: 0.2461 || Val Loss: 3.4767 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 266/500] Train Loss: 3.2374 | Train Acc: 0.2457 || Val Loss: 3.4710 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 267/500] Train Loss: 3.2358 | Train Acc: 0.2519 || Val Loss: 3.4681 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 268/500] Train Loss: 3.2370 | Train Acc: 0.2291 || Val Loss: 3.4700 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 269/500] Train Loss: 3.2348 | Train Acc: 0.2398 || Val Loss: 3.4767 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 270/500] Train Loss: 3.2368 | Train Acc: 0.2467 || Val Loss: 3.4823 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 271/500] Train Loss: 3.2400 | Train Acc: 0.2458 || Val Loss: 3.4753 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 272/500] Train Loss: 3.2332 | Train Acc: 0.2464 || Val Loss: 3.4710 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 273/500] Train Loss: 3.2367 | Train Acc: 0.2348 || Val Loss: 3.4722 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 274/500] Train Loss: 3.2371 | Train Acc: 0.2359 || Val Loss: 3.4759 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 275/500] Train Loss: 3.2337 | Train Acc: 0.2452 || Val Loss: 3.4825 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 276/500] Train Loss: 3.2354 | Train Acc: 0.2479 || Val Loss: 3.4838 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 277/500] Train Loss: 3.2336 | Train Acc: 0.2438 || Val Loss: 3.4808 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 278/500] Train Loss: 3.2327 | Train Acc: 0.2429 || Val Loss: 3.4796 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 279/500] Train Loss: 3.2329 | Train Acc: 0.2403 || Val Loss: 3.4831 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 280/500] Train Loss: 3.2334 | Train Acc: 0.2410 || Val Loss: 3.4855 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 281/500] Train Loss: 3.2329 | Train Acc: 0.2479 || Val Loss: 3.4884 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 282/500] Train Loss: 3.2325 | Train Acc: 0.2429 || Val Loss: 3.4900 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 283/500] Train Loss: 3.2324 | Train Acc: 0.2435 || Val Loss: 3.4928 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 284/500] Train Loss: 3.2309 | Train Acc: 0.2428 || Val Loss: 3.4977 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 285/500] Train Loss: 3.2327 | Train Acc: 0.2427 || Val Loss: 3.5004 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 286/500] Train Loss: 3.2307 | Train Acc: 0.2364 || Val Loss: 3.5030 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 287/500] Train Loss: 3.2312 | Train Acc: 0.2476 || Val Loss: 3.5019 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 288/500] Train Loss: 3.2316 | Train Acc: 0.2402 || Val Loss: 3.5049 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 289/500] Train Loss: 3.2311 | Train Acc: 0.2391 || Val Loss: 3.5089 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 290/500] Train Loss: 3.2284 | Train Acc: 0.2405 || Val Loss: 3.5207 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 291/500] Train Loss: 3.2295 | Train Acc: 0.2421 || Val Loss: 3.5191 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 292/500] Train Loss: 3.2291 | Train Acc: 0.2342 || Val Loss: 3.5158 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 293/500] Train Loss: 3.2277 | Train Acc: 0.2411 || Val Loss: 3.5220 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 294/500] Train Loss: 3.2275 | Train Acc: 0.2400 || Val Loss: 3.5346 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 295/500] Train Loss: 3.2293 | Train Acc: 0.2434 || Val Loss: 3.5314 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 296/500] Train Loss: 3.2272 | Train Acc: 0.2413 || Val Loss: 3.5258 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 297/500] Train Loss: 3.2266 | Train Acc: 0.2353 || Val Loss: 3.5423 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 298/500] Train Loss: 3.2265 | Train Acc: 0.2398 || Val Loss: 3.5386 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 299/500] Train Loss: 3.2248 | Train Acc: 0.2435 || Val Loss: 3.5255 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 300/500] Train Loss: 3.2259 | Train Acc: 0.2319 || Val Loss: 3.5418 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 301/500] Train Loss: 3.2255 | Train Acc: 0.2369 || Val Loss: 3.5572 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 302/500] Train Loss: 3.2250 | Train Acc: 0.2429 || Val Loss: 3.5314 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 303/500] Train Loss: 3.2263 | Train Acc: 0.2333 || Val Loss: 3.5505 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 304/500] Train Loss: 3.2196 | Train Acc: 0.2468 || Val Loss: 3.5615 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 305/500] Train Loss: 3.2223 | Train Acc: 0.2424 || Val Loss: 3.5445 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 306/500] Train Loss: 3.2281 | Train Acc: 0.2406 || Val Loss: 3.5183 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 307/500] Train Loss: 3.2309 | Train Acc: 0.2302 || Val Loss: 3.6183 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 308/500] Train Loss: 3.2326 | Train Acc: 0.2479 || Val Loss: 3.5640 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 309/500] Train Loss: 3.2356 | Train Acc: 0.2324 || Val Loss: 3.5169 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 310/500] Train Loss: 3.2345 | Train Acc: 0.2240 || Val Loss: 3.5704 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 311/500] Train Loss: 3.2267 | Train Acc: 0.2472 || Val Loss: 3.5838 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 312/500] Train Loss: 3.2257 | Train Acc: 0.2482 || Val Loss: 3.5412 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 313/500] Train Loss: 3.2273 | Train Acc: 0.2284 || Val Loss: 3.5409 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 314/500] Train Loss: 3.2261 | Train Acc: 0.2121 || Val Loss: 3.5815 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 315/500] Train Loss: 3.2331 | Train Acc: 0.2472 || Val Loss: 3.5369 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 316/500] Train Loss: 3.2262 | Train Acc: 0.2337 || Val Loss: 3.5313 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 317/500] Train Loss: 3.2200 | Train Acc: 0.2438 || Val Loss: 3.5573 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 318/500] Train Loss: 3.2231 | Train Acc: 0.2320 || Val Loss: 3.5702 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 319/500] Train Loss: 3.2221 | Train Acc: 0.2403 || Val Loss: 3.5628 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 320/500] Train Loss: 3.2212 | Train Acc: 0.2388 || Val Loss: 3.5438 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 321/500] Train Loss: 3.2150 | Train Acc: 0.2479 || Val Loss: 3.5383 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 322/500] Train Loss: 3.2197 | Train Acc: 0.2446 || Val Loss: 3.5680 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 323/500] Train Loss: 3.2146 | Train Acc: 0.2402 || Val Loss: 3.6040 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 324/500] Train Loss: 3.2167 | Train Acc: 0.2369 || Val Loss: 3.6013 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 325/500] Train Loss: 3.2326 | Train Acc: 0.2470 || Val Loss: 3.5247 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 326/500] Train Loss: 3.2200 | Train Acc: 0.2309 || Val Loss: 3.5224 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 327/500] Train Loss: 3.2223 | Train Acc: 0.2403 || Val Loss: 3.5687 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 328/500] Train Loss: 3.2159 | Train Acc: 0.2453 || Val Loss: 3.6058 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 329/500] Train Loss: 3.2189 | Train Acc: 0.2479 || Val Loss: 3.5992 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 330/500] Train Loss: 3.2162 | Train Acc: 0.2345 || Val Loss: 3.5765 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 331/500] Train Loss: 3.2150 | Train Acc: 0.2359 || Val Loss: 3.5853 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 332/500] Train Loss: 3.2151 | Train Acc: 0.2421 || Val Loss: 3.6180 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 333/500] Train Loss: 3.2130 | Train Acc: 0.2457 || Val Loss: 3.6346 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 334/500] Train Loss: 3.2291 | Train Acc: 0.2463 || Val Loss: 3.5699 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 335/500] Train Loss: 3.2144 | Train Acc: 0.2237 || Val Loss: 3.5765 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 336/500] Train Loss: 3.2149 | Train Acc: 0.2313 || Val Loss: 3.6314 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 337/500] Train Loss: 3.2111 | Train Acc: 0.2481 || Val Loss: 3.6466 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 338/500] Train Loss: 3.2432 | Train Acc: 0.2458 || Val Loss: 3.5379 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 339/500] Train Loss: 3.2324 | Train Acc: 0.2340 || Val Loss: 3.5084 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 340/500] Train Loss: 3.2336 | Train Acc: 0.2290 || Val Loss: 3.5350 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 341/500] Train Loss: 3.2227 | Train Acc: 0.2471 || Val Loss: 3.6060 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 342/500] Train Loss: 3.2170 | Train Acc: 0.2468 || Val Loss: 3.6399 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 343/500] Train Loss: 3.2196 | Train Acc: 0.2371 || Val Loss: 3.6305 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 344/500] Train Loss: 3.2248 | Train Acc: 0.2348 || Val Loss: 3.5967 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 345/500] Train Loss: 3.2145 | Train Acc: 0.2161 || Val Loss: 3.5775 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 346/500] Train Loss: 3.2157 | Train Acc: 0.2461 || Val Loss: 3.5838 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 347/500] Train Loss: 3.2204 | Train Acc: 0.2457 || Val Loss: 3.6259 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 348/500] Train Loss: 3.2125 | Train Acc: 0.2434 || Val Loss: 3.6761 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 349/500] Train Loss: 3.2204 | Train Acc: 0.2431 || Val Loss: 3.6557 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 350/500] Train Loss: 3.2368 | Train Acc: 0.2265 || Val Loss: 3.5793 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 351/500] Train Loss: 3.2150 | Train Acc: 0.2244 || Val Loss: 3.5973 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 352/500] Train Loss: 3.2104 | Train Acc: 0.2441 || Val Loss: 3.6536 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 353/500] Train Loss: 3.2208 | Train Acc: 0.2476 || Val Loss: 3.6448 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 354/500] Train Loss: 3.2094 | Train Acc: 0.2489 || Val Loss: 3.6344 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 355/500] Train Loss: 3.2261 | Train Acc: 0.2367 || Val Loss: 3.6112 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 356/500] Train Loss: 3.2257 | Train Acc: 0.2169 || Val Loss: 3.6489 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 357/500] Train Loss: 3.2103 | Train Acc: 0.2357 || Val Loss: 3.7240 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 358/500] Train Loss: 3.2254 | Train Acc: 0.2468 || Val Loss: 3.6117 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 359/500] Train Loss: 3.2225 | Train Acc: 0.2366 || Val Loss: 3.5549 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 360/500] Train Loss: 3.2203 | Train Acc: 0.2359 || Val Loss: 3.6166 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 361/500] Train Loss: 3.2100 | Train Acc: 0.2381 || Val Loss: 3.7436 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 362/500] Train Loss: 3.2134 | Train Acc: 0.2450 || Val Loss: 3.7731 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 363/500] Train Loss: 3.2085 | Train Acc: 0.2432 || Val Loss: 3.7325 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 364/500] Train Loss: 3.2153 | Train Acc: 0.2294 || Val Loss: 3.6979 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 365/500] Train Loss: 3.2057 | Train Acc: 0.2416 || Val Loss: 3.6900 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 366/500] Train Loss: 3.2283 | Train Acc: 0.2391 || Val Loss: 3.5876 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 367/500] Train Loss: 3.2196 | Train Acc: 0.2443 || Val Loss: 3.6342 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 368/500] Train Loss: 3.2090 | Train Acc: 0.2370 || Val Loss: 3.8053 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 369/500] Train Loss: 3.2054 | Train Acc: 0.2375 || Val Loss: 3.9261 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 370/500] Train Loss: 3.2072 | Train Acc: 0.2413 || Val Loss: 3.8693 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 371/500] Train Loss: 3.2163 | Train Acc: 0.2186 || Val Loss: 3.7067 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 372/500] Train Loss: 3.2228 | Train Acc: 0.2306 || Val Loss: 3.5860 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 373/500] Train Loss: 3.2143 | Train Acc: 0.2536 || Val Loss: 3.6280 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 374/500] Train Loss: 3.2199 | Train Acc: 0.2494 || Val Loss: 3.7184 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 375/500] Train Loss: 3.2111 | Train Acc: 0.2479 || Val Loss: 3.8663 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 376/500] Train Loss: 3.2026 | Train Acc: 0.2366 || Val Loss: 4.0136 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 377/500] Train Loss: 3.2334 | Train Acc: 0.2257 || Val Loss: 3.8332 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 378/500] Train Loss: 3.2111 | Train Acc: 0.2301 || Val Loss: 3.7356 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 379/500] Train Loss: 3.2039 | Train Acc: 0.2345 || Val Loss: 3.7211 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 380/500] Train Loss: 3.2176 | Train Acc: 0.2472 || Val Loss: 3.6466 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 381/500] Train Loss: 3.2166 | Train Acc: 0.2471 || Val Loss: 3.6211 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 382/500] Train Loss: 3.2101 | Train Acc: 0.2341 || Val Loss: 3.8146 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 383/500] Train Loss: 3.2018 | Train Acc: 0.2294 || Val Loss: 4.1583 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 384/500] Train Loss: 3.2091 | Train Acc: 0.2373 || Val Loss: 4.0886 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 385/500] Train Loss: 3.2051 | Train Acc: 0.2382 || Val Loss: 3.8138 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 386/500] Train Loss: 3.2166 | Train Acc: 0.2316 || Val Loss: 3.6453 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 387/500] Train Loss: 3.2061 | Train Acc: 0.2393 || Val Loss: 3.6769 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 388/500] Train Loss: 3.2153 | Train Acc: 0.2454 || Val Loss: 3.7430 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 389/500] Train Loss: 3.2105 | Train Acc: 0.2471 || Val Loss: 3.8908 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 390/500] Train Loss: 3.2095 | Train Acc: 0.2456 || Val Loss: 3.9924 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 391/500] Train Loss: 3.2036 | Train Acc: 0.2272 || Val Loss: 4.1939 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 392/500] Train Loss: 3.2093 | Train Acc: 0.2279 || Val Loss: 4.1781 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 393/500] Train Loss: 3.2144 | Train Acc: 0.2351 || Val Loss: 3.8283 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 394/500] Train Loss: 3.2033 | Train Acc: 0.2377 || Val Loss: 3.7264 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 395/500] Train Loss: 3.2070 | Train Acc: 0.2439 || Val Loss: 3.8216 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 396/500] Train Loss: 3.2019 | Train Acc: 0.2476 || Val Loss: 3.9695 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 397/500] Train Loss: 3.1974 | Train Acc: 0.2463 || Val Loss: 4.0404 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 398/500] Train Loss: 3.1988 | Train Acc: 0.2279 || Val Loss: 4.0536 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 399/500] Train Loss: 3.2031 | Train Acc: 0.2214 || Val Loss: 4.1075 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 400/500] Train Loss: 3.1999 | Train Acc: 0.2335 || Val Loss: 4.0755 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 401/500] Train Loss: 3.1963 | Train Acc: 0.2463 || Val Loss: 3.9925 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 402/500] Train Loss: 3.1985 | Train Acc: 0.2478 || Val Loss: 3.9903 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 403/500] Train Loss: 3.2033 | Train Acc: 0.2413 || Val Loss: 4.0265 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 404/500] Train Loss: 3.2019 | Train Acc: 0.2398 || Val Loss: 4.1145 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 405/500] Train Loss: 3.1965 | Train Acc: 0.2395 || Val Loss: 4.3829 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 406/500] Train Loss: 3.1953 | Train Acc: 0.2409 || Val Loss: 4.3471 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 407/500] Train Loss: 3.2088 | Train Acc: 0.2398 || Val Loss: 3.8930 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 408/500] Train Loss: 3.1988 | Train Acc: 0.2435 || Val Loss: 3.9862 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 409/500] Train Loss: 3.2023 | Train Acc: 0.2447 || Val Loss: 4.2149 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 410/500] Train Loss: 3.2050 | Train Acc: 0.2475 || Val Loss: 4.2272 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 411/500] Train Loss: 3.2027 | Train Acc: 0.2402 || Val Loss: 4.0972 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 412/500] Train Loss: 3.1964 | Train Acc: 0.2334 || Val Loss: 4.4001 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 413/500] Train Loss: 3.2031 | Train Acc: 0.2342 || Val Loss: 4.4055 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 414/500] Train Loss: 3.1888 | Train Acc: 0.2429 || Val Loss: 4.3179 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 415/500] Train Loss: 3.1886 | Train Acc: 0.2470 || Val Loss: 4.3077 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 416/500] Train Loss: 3.1896 | Train Acc: 0.2414 || Val Loss: 4.4454 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 417/500] Train Loss: 3.1898 | Train Acc: 0.2416 || Val Loss: 4.3384 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 418/500] Train Loss: 3.1861 | Train Acc: 0.2373 || Val Loss: 4.3146 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 419/500] Train Loss: 3.1846 | Train Acc: 0.2452 || Val Loss: 4.4994 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 420/500] Train Loss: 3.2031 | Train Acc: 0.2424 || Val Loss: 4.1388 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 421/500] Train Loss: 3.1980 | Train Acc: 0.2479 || Val Loss: 4.2823 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 422/500] Train Loss: 3.1935 | Train Acc: 0.2476 || Val Loss: 5.2131 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 423/500] Train Loss: 3.2113 | Train Acc: 0.2410 || Val Loss: 4.1431 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 424/500] Train Loss: 3.2033 | Train Acc: 0.2317 || Val Loss: 4.0359 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 425/500] Train Loss: 3.2013 | Train Acc: 0.2407 || Val Loss: 5.0304 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 426/500] Train Loss: 3.2018 | Train Acc: 0.2464 || Val Loss: 5.1654 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 427/500] Train Loss: 3.1933 | Train Acc: 0.2457 || Val Loss: 4.6224 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 428/500] Train Loss: 3.1838 | Train Acc: 0.2388 || Val Loss: 4.5629 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 429/500] Train Loss: 3.1865 | Train Acc: 0.2346 || Val Loss: 4.7351 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 430/500] Train Loss: 3.1888 | Train Acc: 0.2367 || Val Loss: 4.5132 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 431/500] Train Loss: 3.1856 | Train Acc: 0.2475 || Val Loss: 4.3364 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 432/500] Train Loss: 3.1879 | Train Acc: 0.2542 || Val Loss: 4.5878 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 433/500] Train Loss: 3.1832 | Train Acc: 0.2413 || Val Loss: 5.0617 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 434/500] Train Loss: 3.1849 | Train Acc: 0.2418 || Val Loss: 4.9815 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 435/500] Train Loss: 3.1827 | Train Acc: 0.2338 || Val Loss: 4.6885 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 436/500] Train Loss: 3.1852 | Train Acc: 0.2432 || Val Loss: 4.9253 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 437/500] Train Loss: 3.1804 | Train Acc: 0.2453 || Val Loss: 5.1043 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 438/500] Train Loss: 3.1970 | Train Acc: 0.2460 || Val Loss: 4.3430 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 439/500] Train Loss: 3.1973 | Train Acc: 0.2435 || Val Loss: 4.2818 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 440/500] Train Loss: 3.1920 | Train Acc: 0.2470 || Val Loss: 5.6833 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 441/500] Train Loss: 3.1865 | Train Acc: 0.2441 || Val Loss: 5.7268 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 442/500] Train Loss: 3.2017 | Train Acc: 0.2304 || Val Loss: 4.0384 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 443/500] Train Loss: 3.2003 | Train Acc: 0.2341 || Val Loss: 4.1857 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 444/500] Train Loss: 3.1894 | Train Acc: 0.2460 || Val Loss: 5.6138 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 445/500] Train Loss: 3.1885 | Train Acc: 0.2474 || Val Loss: 5.5021 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 446/500] Train Loss: 3.1900 | Train Acc: 0.2460 || Val Loss: 4.3221 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 447/500] Train Loss: 3.2031 | Train Acc: 0.2405 || Val Loss: 4.0567 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 448/500] Train Loss: 3.1976 | Train Acc: 0.2375 || Val Loss: 4.8902 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 449/500] Train Loss: 3.1796 | Train Acc: 0.2504 || Val Loss: 5.9598 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 450/500] Train Loss: 3.2080 | Train Acc: 0.2468 || Val Loss: 5.2082 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 451/500] Train Loss: 3.1772 | Train Acc: 0.2553 || Val Loss: 4.4488 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 452/500] Train Loss: 3.1857 | Train Acc: 0.2528 || Val Loss: 4.6640 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 453/500] Train Loss: 3.1849 | Train Acc: 0.2364 || Val Loss: 5.2969 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 454/500] Train Loss: 3.1781 | Train Acc: 0.2395 || Val Loss: 5.5355 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 455/500] Train Loss: 3.2040 | Train Acc: 0.2411 || Val Loss: 4.7066 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 456/500] Train Loss: 3.1892 | Train Acc: 0.2499 || Val Loss: 4.1352 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 457/500] Train Loss: 3.1931 | Train Acc: 0.2529 || Val Loss: 4.4446 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 458/500] Train Loss: 3.1789 | Train Acc: 0.2518 || Val Loss: 5.4840 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 459/500] Train Loss: 3.2074 | Train Acc: 0.2338 || Val Loss: 5.5088 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 460/500] Train Loss: 3.1767 | Train Acc: 0.2476 || Val Loss: 5.2504 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 461/500] Train Loss: 3.1739 | Train Acc: 0.2472 || Val Loss: 5.0698 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 462/500] Train Loss: 3.1756 | Train Acc: 0.2497 || Val Loss: 5.1482 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 463/500] Train Loss: 3.1729 | Train Acc: 0.2512 || Val Loss: 5.3406 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 464/500] Train Loss: 3.1708 | Train Acc: 0.2515 || Val Loss: 5.3983 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 465/500] Train Loss: 3.1916 | Train Acc: 0.2399 || Val Loss: 4.9047 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 466/500] Train Loss: 3.1741 | Train Acc: 0.2478 || Val Loss: 4.6213 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 467/500] Train Loss: 3.1729 | Train Acc: 0.2550 || Val Loss: 4.9844 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 468/500] Train Loss: 3.1726 | Train Acc: 0.2504 || Val Loss: 5.5427 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 469/500] Train Loss: 3.1722 | Train Acc: 0.2515 || Val Loss: 5.6834 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 470/500] Train Loss: 3.1771 | Train Acc: 0.2450 || Val Loss: 5.1230 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 471/500] Train Loss: 3.1783 | Train Acc: 0.2425 || Val Loss: 4.6919 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 472/500] Train Loss: 3.1758 | Train Acc: 0.2499 || Val Loss: 5.0703 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 473/500] Train Loss: 3.1667 | Train Acc: 0.2544 || Val Loss: 5.8020 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 474/500] Train Loss: 3.1691 | Train Acc: 0.2506 || Val Loss: 5.8910 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 475/500] Train Loss: 3.1751 | Train Acc: 0.2474 || Val Loss: 5.2604 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 476/500] Train Loss: 3.1656 | Train Acc: 0.2416 || Val Loss: 4.8021 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 477/500] Train Loss: 3.1729 | Train Acc: 0.2411 || Val Loss: 5.1253 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 478/500] Train Loss: 3.1691 | Train Acc: 0.2525 || Val Loss: 5.6823 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 479/500] Train Loss: 3.1673 | Train Acc: 0.2558 || Val Loss: 5.7969 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 480/500] Train Loss: 3.1690 | Train Acc: 0.2507 || Val Loss: 5.3392 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 481/500] Train Loss: 3.1679 | Train Acc: 0.2465 || Val Loss: 5.1143 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 482/500] Train Loss: 3.1718 | Train Acc: 0.2425 || Val Loss: 5.7984 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 483/500] Train Loss: 3.1662 | Train Acc: 0.2503 || Val Loss: 6.0532 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 484/500] Train Loss: 3.1648 | Train Acc: 0.2512 || Val Loss: 5.6724 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 485/500] Train Loss: 3.1595 | Train Acc: 0.2518 || Val Loss: 5.1949 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 486/500] Train Loss: 3.1968 | Train Acc: 0.2446 || Val Loss: 4.4890 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 487/500] Train Loss: 3.1832 | Train Acc: 0.2511 || Val Loss: 5.5853 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 488/500] Train Loss: 3.1635 | Train Acc: 0.2551 || Val Loss: 6.6424 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 489/500] Train Loss: 3.2432 | Train Acc: 0.2439 || Val Loss: 4.7499 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 490/500] Train Loss: 3.1842 | Train Acc: 0.2460 || Val Loss: 4.3063 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 491/500] Train Loss: 3.1994 | Train Acc: 0.2399 || Val Loss: 5.6809 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 492/500] Train Loss: 3.1702 | Train Acc: 0.2550 || Val Loss: 6.7712 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 493/500] Train Loss: 3.2039 | Train Acc: 0.2517 || Val Loss: 6.2725 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 494/500] Train Loss: 3.1672 | Train Acc: 0.2460 || Val Loss: 5.1039 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 495/500] Train Loss: 3.1667 | Train Acc: 0.2348 || Val Loss: 4.6241 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 496/500] Train Loss: 3.1774 | Train Acc: 0.2438 || Val Loss: 5.1913 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 497/500] Train Loss: 3.1646 | Train Acc: 0.2539 || Val Loss: 6.0969 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 498/500] Train Loss: 3.1665 | Train Acc: 0.2508 || Val Loss: 6.3988 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 499/500] Train Loss: 3.1726 | Train Acc: 0.2452 || Val Loss: 5.9910 | Val Acc: 0.2442\n",
      "Tokenizer says vocab size = 1837\n",
      "You currently set vocab_size = 2000 in the model.\n",
      "[Epoch 500/500] Train Loss: 3.1651 | Train Acc: 0.2470 || Val Loss: 5.2160 | Val Acc: 0.2442\n",
      "Training complete. Loading best model weights from checkpoint.\n",
      "Test Full-Sequence Accuracy: 0.0000\n",
      "Average Edit-Distance Score (1=identical): 0.2450\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "###############################################################################\n",
    "# 1. Dataset and Collation\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "\n",
    "class TokenSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset for pre-tokenized sequences stored in memory. \n",
    "    Each sequence is a list of integer token IDs.\n",
    "\n",
    "    This class returns (input_seq, target_seq), where:\n",
    "      - input_seq = sequence[:-1]\n",
    "      - target_seq = sequence[1:]\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences: List[List[int]]):\n",
    "        super().__init__()\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[List[int], List[int]]:\n",
    "        seq = self.sequences[idx]\n",
    "        # We shift to create input/target pairs for next-token prediction\n",
    "        input_seq = seq[:-1]\n",
    "        target_seq = seq[1:]\n",
    "        return input_seq, target_seq\n",
    "\n",
    "\n",
    "class PadCollator:\n",
    "    \"\"\"\n",
    "    A custom collator that pads input sequences to the same length in a batch,\n",
    "    creating attention masks and ensuring alignment of input/target sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, pad_token_id: int = 0):\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __call__(self, batch: List[Tuple[List[int], List[int]]]):\n",
    "        # Extract all input/target pairs\n",
    "        input_batch, target_batch = zip(*batch)\n",
    "\n",
    "        max_len = max(len(seq) for seq in input_batch)\n",
    "\n",
    "        padded_inputs = []\n",
    "        padded_targets = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for inp, tgt in zip(input_batch, target_batch):\n",
    "            inp_len = len(inp)\n",
    "            pad_len = max_len - inp_len\n",
    "\n",
    "            # Pad inputs & targets\n",
    "            padded_inp = inp + [self.pad_token_id] * pad_len\n",
    "            padded_tgt = tgt + [self.pad_token_id] * pad_len\n",
    "\n",
    "            # Create attention mask: 1 for real tokens, 0 for padded\n",
    "            att_mask = [1] * inp_len + [0] * pad_len\n",
    "\n",
    "            padded_inputs.append(padded_inp)\n",
    "            padded_targets.append(padded_tgt)\n",
    "            attention_masks.append(att_mask)\n",
    "\n",
    "        # Convert to tensors\n",
    "        padded_inputs = torch.tensor(padded_inputs, dtype=torch.long)\n",
    "        padded_targets = torch.tensor(padded_targets, dtype=torch.long)\n",
    "        attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "\n",
    "        return padded_inputs, padded_targets, attention_masks\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2. Model Definition\n",
    "###############################################################################\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard sinusoidal positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 50000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x shape: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # Add positional encoding\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A causal (decoder-only) Transformer model for next-token prediction.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 10,\n",
    "        dim_feedforward: int = 1024,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Token Embedding\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "\n",
    "        # Transformer Decoder Layers\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Final projection to vocabulary\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Causal mask cache (for efficiency)\n",
    "        self.register_buffer(\"mask_cache\", None)\n",
    "\n",
    "    def _generate_causal_mask(self, sz: int, device: torch.device):\n",
    "        \"\"\"\n",
    "        Generates an upper-triangular causal mask to ensure each token \n",
    "        can only attend to preceding tokens (including itself).\n",
    "        \"\"\"\n",
    "        if (self.mask_cache is None) or (self.mask_cache.size(0) < sz):\n",
    "            mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1)\n",
    "            # Convert to boolean, True means \"block this position\"\n",
    "            mask = mask.bool()\n",
    "            self.mask_cache = mask\n",
    "        else:\n",
    "            mask = self.mask_cache[:sz, :sz]\n",
    "        return mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_ids: (batch, seq_len)\n",
    "        attention_mask: (batch, seq_len) => 1 for real tokens, 0 for pads\n",
    "\n",
    "        Returns:\n",
    "          logits of shape (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        device = input_ids.device\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        # Generate token embeddings\n",
    "        tok_emb = self.token_emb(input_ids)  # (batch, seq_len, d_model)\n",
    "        # Add positional encoding\n",
    "        pos_emb = self.pos_encoder(tok_emb)\n",
    "\n",
    "        # Prepare the causal mask\n",
    "        causal_mask = self._generate_causal_mask(seq_len, device=device)  # (seq_len, seq_len)\n",
    "\n",
    "        # We also need to expand the attention_mask to shape (batch, 1, seq_len)\n",
    "        # so it can be broadcast to (batch, seq_len, seq_len).\n",
    "        # We'll turn 0 => True in the mask to block those positions.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # So the final mask used by the decoder is (batch, seq_len, seq_len)\n",
    "        # with True where we want to block attention.\n",
    "        combined_mask = causal_mask.unsqueeze(0) | (extended_attention_mask == 0)\n",
    "\n",
    "        # Permute to fit PyTorch's (seq_len, batch, d_model)\n",
    "        pos_emb = pos_emb.permute(1, 0, 2)  # => (seq_len, batch, d_model)\n",
    "\n",
    "        # Decode (TransformerDecoder expects shape (seq_len, batch, d_model))\n",
    "        # The \"memory\" here is empty because we're using a decoder-only approach.\n",
    "        decoded = self.transformer_decoder(\n",
    "            pos_emb,\n",
    "            memory=torch.zeros(0, batch_size, self.d_model, device=device),  # dummy empty memory\n",
    "            tgt_mask=combined_mask[0],  # shape (seq_len, seq_len) for a single batch? We'll do a trick below.\n",
    "            # tgt_key_padding_mask=~attention_mask.bool()  # shape (batch, seq_len)\n",
    "             tgt_key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "\n",
    "        # NOTE: PyTorch's TransformerDecoder can’t directly handle a 3D mask. \n",
    "        # We used 'tgt_key_padding_mask' for pad tokens and 'tgt_mask' for causality. \n",
    "        # This approach merges them. If you have more advanced needs, you'd implement a custom layer or reshape.\n",
    "\n",
    "        # Undo the permute: (seq_len, batch, d_model) -> (batch, seq_len, d_model)\n",
    "        decoded = decoded.permute(1, 0, 2).contiguous()\n",
    "\n",
    "        # Project to vocab\n",
    "        logits = self.output_proj(decoded)  # (batch, seq_len, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. Training / Validation / Testing\n",
    "###############################################################################\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Train for one epoch. Returns (avg_loss, approx_token_accuracy).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets, attention_mask) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs, attention_mask)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "        # Reshape for loss: (batch*seq_len, vocab_size) vs (batch*seq_len)\n",
    "        vocab_size = logits.size(-1)\n",
    "        loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        try:\n",
    "            real_vocab_size = tokenizer.get_vocab_size()\n",
    "        except:\n",
    "        # For older versions of Tokenizers, you might need:\n",
    "            real_vocab_size = len(tokenizer.get_vocab())\n",
    "\n",
    "        print(f\"Tokenizer says vocab size = {real_vocab_size}\")\n",
    "\n",
    "        # Now compare that with your currently hard-coded vocab_size\n",
    "        # For example:\n",
    "        print(f\"You currently set vocab_size = 2000 in the model.\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate stats\n",
    "        running_loss += loss.item()\n",
    "        # Approximate token-level accuracy\n",
    "        preds = logits.argmax(dim=-1)  # (batch, seq_len)\n",
    "        mask_flat = attention_mask.view(-1).bool()\n",
    "        correct = (preds.view(-1)[mask_flat] == targets.view(-1)[mask_flat]).sum().item()\n",
    "        count = mask_flat.sum().item()\n",
    "        running_correct += correct\n",
    "        total_tokens += count\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_acc = running_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def validate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Validate the model. Returns (avg_loss, approx_token_accuracy).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, attention_mask in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            logits = model(inputs, attention_mask)\n",
    "\n",
    "            vocab_size = logits.size(-1)\n",
    "            loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Approximate token-level accuracy\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask_flat = attention_mask.view(-1).bool()\n",
    "            correct = (preds.view(-1)[mask_flat] == targets.view(-1)[mask_flat]).sum().item()\n",
    "            count = mask_flat.sum().item()\n",
    "            running_correct += correct\n",
    "            total_tokens += count\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_acc = running_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def test_sequence_accuracy(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the fraction of sequences where the entire predicted sequence\n",
    "    matches the target sequence exactly. \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_sequences = 0\n",
    "    total_sequences = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, attention_mask in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            logits = model(inputs, attention_mask)\n",
    "            preds = logits.argmax(dim=-1)  # (batch, seq_len)\n",
    "\n",
    "            # For each sequence in the batch, compare all tokens (where attention_mask=1).\n",
    "            for i in range(inputs.size(0)):\n",
    "                seq_mask = attention_mask[i].bool()\n",
    "                pred_seq = preds[i, seq_mask]\n",
    "                tgt_seq = targets[i, seq_mask]\n",
    "                total_sequences += 1\n",
    "                if torch.equal(pred_seq, tgt_seq):\n",
    "                    correct_sequences += 1\n",
    "\n",
    "    return correct_sequences / total_sequences if total_sequences > 0 else 0.0\n",
    "\n",
    "def test_edit_distance_score(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the average edit-distance-based similarity score across all sequences \n",
    "    in the test set. For each sequence:\n",
    "       score = 1 - (levenshtein_distance / max_len_of_seq).\n",
    "    Then we average those scores over the entire test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_score = 0.0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, attention_mask in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            logits = model(inputs, attention_mask)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "\n",
    "            # For each sequence in the batch, compare predicted vs. target tokens \n",
    "            # (only the real tokens where attention_mask=1).\n",
    "            batch_size = inputs.size(0)\n",
    "            for i in range(batch_size):\n",
    "                seq_mask = attention_mask[i].bool()\n",
    "                pred_seq = preds[i, seq_mask].cpu().tolist()\n",
    "                tgt_seq = targets[i, seq_mask].cpu().tolist()\n",
    "\n",
    "                score = edit_distance_score(pred_seq, tgt_seq)\n",
    "                total_score += score\n",
    "                total_count += 1\n",
    "\n",
    "    return total_score / total_count if total_count > 0 else 0.0\n",
    "\n",
    "###############################################################################\n",
    "# 4. Main Script\n",
    "###############################################################################\n",
    "def main():\n",
    "    # ---------------------------\n",
    "    # 4.1 Load Tokenized Data\n",
    "    # ---------------------------\n",
    "    json_path = \"dataset_encoded.json\"  # Change to your path if needed\n",
    "    with open(json_path, \"r\") as f:\n",
    "        token_data_dict = json.load(f)\n",
    "\n",
    "    # token_data_dict is assumed to be {filename: [list_of_token_ids], ...}\n",
    "    # Merge all token lists into one big list if needed, or keep them separate.\n",
    "    # We'll merge them for a single dataset:\n",
    "    all_sequences = []\n",
    "    for seq_list in token_data_dict.values():\n",
    "        # seq_list is presumably a list of ints\n",
    "        # Possibly it's a list of lists if you segmented each file. \n",
    "        # If needed, adapt to your structure. \n",
    "        # We'll assume it's a single list of token IDs per entry.\n",
    "        if isinstance(seq_list[0], int):\n",
    "            # single sequence\n",
    "            all_sequences.append(seq_list)\n",
    "        else:\n",
    "            # multiple sequences in a sub-list\n",
    "            all_sequences.extend(seq_list)\n",
    "\n",
    "    # Filter out any sequences shorter than 2 tokens (otherwise can't do input/target shift).\n",
    "    all_sequences = [seq for seq in all_sequences if len(seq) > 1]\n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 1: Check average sequence length & approximate perfect-sequence probability\n",
    "    ###############################################################################\n",
    "    lengths = [len(seq) for seq in all_sequences]\n",
    "    avg_len = sum(lengths) / len(lengths)\n",
    "    max_len = max(lengths)\n",
    "    print(f\"Number of sequences: {len(all_sequences)}\")\n",
    "    print(f\"Average sequence length: {avg_len:.2f}\")\n",
    "    print(f\"Max sequence length: {max_len}\")\n",
    "\n",
    "    # Suppose token accuracy is around 0.70:\n",
    "    approx_token_acc = 0.70\n",
    "    print(f\"Average Length:{avg_len}\")\n",
    "    approx_full_seq_acc = (approx_token_acc ** avg_len)\n",
    "    print(f\"Estimated fraction fully correct at 70% token acc: {approx_full_seq_acc:.6f}\")\n",
    "\n",
    "    print(\"I want the code to be executed only till here.\")\n",
    "    # raise SystemExit\n",
    "    MAX_CHUNK_LEN = 64  # or any reasonable max length\n",
    "    chunked_sequences = []\n",
    "\n",
    "    for seq_list in token_data_dict.values():\n",
    "        # If seq_list is a single sequence of IDs, chunk it\n",
    "        if isinstance(seq_list[0], int):\n",
    "            seq = seq_list\n",
    "            # Break the single seq into multiple chunks\n",
    "            for i in range(0, len(seq), MAX_CHUNK_LEN):\n",
    "                chunk = seq[i:i+MAX_CHUNK_LEN]\n",
    "                # We only keep chunks that have at least 2 tokens\n",
    "                if len(chunk) > 1:\n",
    "                    chunked_sequences.append(chunk)\n",
    "        else:\n",
    "            # If seq_list is already a list of sequences, chunk each\n",
    "            for seq in seq_list:\n",
    "                for i in range(0, len(seq), MAX_CHUNK_LEN):\n",
    "                    chunk = seq[i:i+MAX_CHUNK_LEN]\n",
    "                    if len(chunk) > 1:\n",
    "                        chunked_sequences.append(chunk)\n",
    "\n",
    "    all_sequences = chunked_sequences\n",
    "\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4.2 Split into Train/Val/Test\n",
    "    # ---------------------------\n",
    "    random.shuffle(all_sequences)\n",
    "    dataset_size = len(all_sequences)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    val_size = int(0.1 * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    train_data, val_data, test_data = random_split(\n",
    "        all_sequences,\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    train_sequences = list(train_data)\n",
    "    val_sequences = list(val_data)\n",
    "    test_sequences = list(test_data)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 3: Inspect a few example sequences\n",
    "    ###############################################################################\n",
    "    print(\"Sample (raw) sequences from all_sequences:\")\n",
    "    for i in range(3):\n",
    "        print(f\"Example {i}:\", all_sequences[i])\n",
    "        print(\"Length:\", len(all_sequences[i]))\n",
    "        print(\"--------\")\n",
    "    # ---------------------------\n",
    "    # 4.3 Create Datasets & Loaders\n",
    "    # ---------------------------\n",
    "    pad_token_id = 0  # Adjust if your PAD token is something else\n",
    "    train_dataset = TokenSequenceDataset(train_sequences)\n",
    "    val_dataset = TokenSequenceDataset(val_sequences)\n",
    "    test_dataset = TokenSequenceDataset(test_sequences)\n",
    "    \n",
    "    collator = PadCollator(pad_token_id=pad_token_id)\n",
    "    batch_size = 256\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collator)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 4: Evaluate on short sequences only\n",
    "    ###############################################################################\n",
    "    short_test_sequences = [seq for seq in test_sequences if len(seq) <= 8]\n",
    "    print(f\"Number of short test sequences (<= 8 tokens): {len(short_test_sequences)}\")\n",
    "\n",
    "    if len(short_test_sequences) > 0:\n",
    "        short_test_dataset = TokenSequenceDataset(short_test_sequences)\n",
    "        short_test_loader = DataLoader(\n",
    "            short_test_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,\n",
    "            collate_fn=collator\n",
    "        )\n",
    "\n",
    "        # We'll do a quick “test_sequence_accuracy” call on these short ones, once the model is trained\n",
    "        # Just store them for later usage:\n",
    "    else:\n",
    "        short_test_loader = None\n",
    "        print(\"No short sequences to test.\")\n",
    "\n",
    "\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(\"Sample input shape:\", sample_batch[0].shape)\n",
    "    print(\"Sample target shape:\", sample_batch[1].shape)\n",
    "    print(\"Sample mask shape:\", sample_batch[2].shape)\n",
    "    print(\"Sample input:\", sample_batch[0][0][:10])  # First 10 tokens of first example\n",
    "    print(\"Sample target:\", sample_batch[1][0][:10])\n",
    "    print(\"Max token ID in input:\", sample_batch[0].max().item())\n",
    "    print(\"Max token ID in target:\", sample_batch[1].max().item())\n",
    "        # ---------------------------\n",
    "    # 4.4 Model, Optimizer, Loss\n",
    "    # ---------------------------\n",
    "    # You likely know your max vocab size from your tokenizer\n",
    "    # For example, if you used Byte-Level BPE or a custom mapping:\n",
    "    vocab_size = 2000  # <-- Replace with your actual vocabulary size\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = DecoderOnlyTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=512,          # Hidden dimension\n",
    "        nhead=8,              # Number of attention heads\n",
    "        num_layers=10,         # Number of transformer decoder layers\n",
    "        dim_feedforward=1024, # FFN dimension\n",
    "        dropout=0.1\n",
    "    )\n",
    "    model.to(device)    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "    pad_token_id=0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4.5 Training Loop\n",
    "    # ---------------------------\n",
    "    epochs = 500\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- Train ---\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        # --- Validate ---\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"[Epoch {epoch}/{epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} || \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Checkpoint if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"transformer_model.pt\")\n",
    "            print(\"  (Best validation loss so far, saving model)\")\n",
    "\n",
    "    print(\"Training complete. Loading best model weights from checkpoint.\")\n",
    "    model.load_state_dict(torch.load(\"transformer_model.pt\"))\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4.6 Evaluate on Test Set (Sequence Accuracy)\n",
    "    # ---------------------------\n",
    "    seq_acc = test_sequence_accuracy(model, test_loader, device)\n",
    "    print(f\"Test Full-Sequence Accuracy: {seq_acc:.4f}\")\n",
    "\n",
    "    if short_test_loader is not None:\n",
    "        short_seq_acc = test_sequence_accuracy(model, short_test_loader, device)\n",
    "        print(f\"Short-test-set (<=8 tokens) full-sequence accuracy: {short_seq_acc:.4f}\")\n",
    "\n",
    "    # New partial-credit metric\n",
    "    eds_score = test_edit_distance_score(model, test_loader, device)\n",
    "    print(f\"Average Edit-Distance Score (1=identical): {eds_score:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50df46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10837e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713f117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harshenv",
   "language": "python",
   "name": "harshenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
