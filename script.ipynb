{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formulas saved to filenames.txt\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "wb = load_workbook('FeynmanEquations.xlsx', data_only=False)\n",
    "ws = wb.active\n",
    "\n",
    "# Iterate through rows (we start from row 2 because we skip header)\n",
    "formula_col_index = 1 \n",
    "formula_list=[]\n",
    "# Iterate through rows (we start from row 2 because we skip header)\n",
    "for row in ws.iter_rows(min_row=2):\n",
    "    # get cell in formula column\n",
    "    cell = row[formula_col_index - 1]  # zero-indexed\n",
    "    formula_list.append(cell.value) \n",
    "    \n",
    "\n",
    "##Save the formula_list in a text file\n",
    "filename = \"filenames.txt\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for formula in formula_list:\n",
    "        f.write(formula + \"\\n\")\n",
    "\n",
    "print(f\"Formulas saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATA_END_ID = 4  # replace with the actual token ID for [DATA_END]\n",
    "combined_samples = []\n",
    "\n",
    "# 1) Load RPN file\n",
    "with open(\"formulas_rpn.json\", \"r\") as f:\n",
    "    rpn_data = json.load(f)  # a list of dicts: [{\"id\": \"I.6.2a\", \"rpn\": [...]}, ...]\n",
    "\n",
    "# 2) Load numeric-data file\n",
    "with open(\"dataset_encoded.json\", \"r\") as f:\n",
    "    data_dict = json.load(f) # a dict with keys like \"I.6.2a\" : [ data tokens ], etc.\n",
    "\n",
    "# 3) Combine data & RPN for each entry\n",
    "for item in rpn_data:\n",
    "    formula_id = item[\"id\"]\n",
    "    rpn_tokens = item[\"rpn\"]\n",
    "\n",
    "    # Find matching data tokens (same ID) in data_dict\n",
    "    if formula_id in data_dict:\n",
    "        data_tokens = data_dict[formula_id]\n",
    "        combined = data_tokens + [DATA_END_ID] + rpn_tokens\n",
    "        combined_samples.append(combined)\n",
    "    else:\n",
    "        print(f\"Warning: {formula_id} not found in dataset_encoded.json\")\n",
    "\n",
    "# 'combined_samples' now contains your full sequences, each with data followed by [DATA_END] and formula RPN tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"combined_samples.json\", \"w\") as f:\n",
    "    json.dump(combined_samples, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problems - Inconsistency in the dataset \n",
    "## In the excel file I.15.1 is there while in the files its labelled as I.15.10\n",
    "## Similary in the files instead of I.48.2 its labelled as I.48.20 which cause inconsitencies in the dataset\n",
    "### Also II.11.17's file is incorrectly labelled as II.11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input shape: torch.Size([256, 31])\n",
      "Sample target shape: torch.Size([256, 31])\n",
      "Sample mask shape: torch.Size([256, 31])\n",
      "Sample input: tensor([ 647,  968, 1731,    2,  647,  983, 1731,    2,  647,  976])\n",
      "Sample target: tensor([ 968, 1731,    2,  647,  983, 1731,    2,  647,  976, 1731])\n",
      "Max token ID in input: 1733\n",
      "Max token ID in target: 1733\n",
      "[Epoch 1/50] Train Loss: 2.0502 | Train Acc: 0.6656 || Val Loss: 1.8127 | Val Acc: 0.6850\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 2/50] Train Loss: 1.8016 | Train Acc: 0.6849 || Val Loss: 1.7787 | Val Acc: 0.6888\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 3/50] Train Loss: 1.7802 | Train Acc: 0.6889 || Val Loss: 1.7639 | Val Acc: 0.6922\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 4/50] Train Loss: 1.7636 | Train Acc: 0.6928 || Val Loss: 1.7508 | Val Acc: 0.6947\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 5/50] Train Loss: 1.7495 | Train Acc: 0.6951 || Val Loss: 1.7364 | Val Acc: 0.6977\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 6/50] Train Loss: 1.7383 | Train Acc: 0.6971 || Val Loss: 1.7291 | Val Acc: 0.6991\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 7/50] Train Loss: 1.7302 | Train Acc: 0.6983 || Val Loss: 1.7236 | Val Acc: 0.7000\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 8/50] Train Loss: 1.7241 | Train Acc: 0.6995 || Val Loss: 1.7195 | Val Acc: 0.7005\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 9/50] Train Loss: 1.7184 | Train Acc: 0.7002 || Val Loss: 1.7163 | Val Acc: 0.7017\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 10/50] Train Loss: 1.7134 | Train Acc: 0.7010 || Val Loss: 1.7125 | Val Acc: 0.7026\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 11/50] Train Loss: 1.7089 | Train Acc: 0.7017 || Val Loss: 1.7119 | Val Acc: 0.7024\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 12/50] Train Loss: 1.7054 | Train Acc: 0.7023 || Val Loss: 1.7092 | Val Acc: 0.7034\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 13/50] Train Loss: 1.7019 | Train Acc: 0.7026 || Val Loss: 1.7096 | Val Acc: 0.7033\n",
      "[Epoch 14/50] Train Loss: 1.6984 | Train Acc: 0.7032 || Val Loss: 1.7084 | Val Acc: 0.7032\n",
      "  (Best validation loss so far, saving model)\n",
      "[Epoch 15/50] Train Loss: 1.6954 | Train Acc: 0.7036 || Val Loss: 1.7090 | Val Acc: 0.7034\n",
      "[Epoch 16/50] Train Loss: 1.6919 | Train Acc: 0.7040 || Val Loss: 1.7084 | Val Acc: 0.7039\n",
      "[Epoch 17/50] Train Loss: 1.6886 | Train Acc: 0.7045 || Val Loss: 1.7093 | Val Acc: 0.7040\n",
      "[Epoch 18/50] Train Loss: 1.6853 | Train Acc: 0.7048 || Val Loss: 1.7102 | Val Acc: 0.7034\n",
      "[Epoch 19/50] Train Loss: 1.6821 | Train Acc: 0.7050 || Val Loss: 1.7105 | Val Acc: 0.7040\n",
      "[Epoch 20/50] Train Loss: 1.6779 | Train Acc: 0.7055 || Val Loss: 1.7122 | Val Acc: 0.7043\n",
      "[Epoch 21/50] Train Loss: 1.6742 | Train Acc: 0.7059 || Val Loss: 1.7163 | Val Acc: 0.7038\n",
      "[Epoch 22/50] Train Loss: 1.6703 | Train Acc: 0.7061 || Val Loss: 1.7162 | Val Acc: 0.7036\n",
      "[Epoch 23/50] Train Loss: 1.6660 | Train Acc: 0.7068 || Val Loss: 1.7172 | Val Acc: 0.7042\n",
      "[Epoch 24/50] Train Loss: 1.6615 | Train Acc: 0.7069 || Val Loss: 1.7232 | Val Acc: 0.7035\n",
      "[Epoch 25/50] Train Loss: 1.6562 | Train Acc: 0.7074 || Val Loss: 1.7262 | Val Acc: 0.7036\n",
      "[Epoch 26/50] Train Loss: 1.6506 | Train Acc: 0.7080 || Val Loss: 1.7273 | Val Acc: 0.7036\n",
      "[Epoch 27/50] Train Loss: 1.6451 | Train Acc: 0.7083 || Val Loss: 1.7312 | Val Acc: 0.7040\n",
      "[Epoch 28/50] Train Loss: 1.6390 | Train Acc: 0.7089 || Val Loss: 1.7363 | Val Acc: 0.7029\n",
      "[Epoch 29/50] Train Loss: 1.6324 | Train Acc: 0.7093 || Val Loss: 1.7363 | Val Acc: 0.7035\n",
      "[Epoch 30/50] Train Loss: 1.6258 | Train Acc: 0.7100 || Val Loss: 1.7425 | Val Acc: 0.7035\n",
      "[Epoch 31/50] Train Loss: 1.6184 | Train Acc: 0.7106 || Val Loss: 1.7512 | Val Acc: 0.7026\n",
      "[Epoch 32/50] Train Loss: 1.6110 | Train Acc: 0.7112 || Val Loss: 1.7586 | Val Acc: 0.7032\n",
      "[Epoch 33/50] Train Loss: 1.6031 | Train Acc: 0.7120 || Val Loss: 1.7620 | Val Acc: 0.7028\n",
      "[Epoch 34/50] Train Loss: 1.5947 | Train Acc: 0.7126 || Val Loss: 1.7693 | Val Acc: 0.7022\n",
      "[Epoch 35/50] Train Loss: 1.5866 | Train Acc: 0.7135 || Val Loss: 1.7772 | Val Acc: 0.7031\n",
      "[Epoch 36/50] Train Loss: 1.5774 | Train Acc: 0.7142 || Val Loss: 1.7819 | Val Acc: 0.7025\n",
      "[Epoch 37/50] Train Loss: 1.5693 | Train Acc: 0.7147 || Val Loss: 1.7906 | Val Acc: 0.7022\n",
      "[Epoch 38/50] Train Loss: 1.5602 | Train Acc: 0.7155 || Val Loss: 1.7977 | Val Acc: 0.7018\n",
      "[Epoch 39/50] Train Loss: 1.5514 | Train Acc: 0.7164 || Val Loss: 1.8048 | Val Acc: 0.7022\n",
      "[Epoch 40/50] Train Loss: 1.5420 | Train Acc: 0.7173 || Val Loss: 1.8154 | Val Acc: 0.7021\n",
      "[Epoch 41/50] Train Loss: 1.5334 | Train Acc: 0.7180 || Val Loss: 1.8220 | Val Acc: 0.7020\n",
      "[Epoch 42/50] Train Loss: 1.5242 | Train Acc: 0.7188 || Val Loss: 1.8368 | Val Acc: 0.7017\n",
      "[Epoch 43/50] Train Loss: 1.5150 | Train Acc: 0.7197 || Val Loss: 1.8401 | Val Acc: 0.7019\n",
      "[Epoch 44/50] Train Loss: 1.5061 | Train Acc: 0.7208 || Val Loss: 1.8443 | Val Acc: 0.7015\n",
      "[Epoch 45/50] Train Loss: 1.4972 | Train Acc: 0.7216 || Val Loss: 1.8533 | Val Acc: 0.7014\n",
      "[Epoch 46/50] Train Loss: 1.4883 | Train Acc: 0.7227 || Val Loss: 1.8615 | Val Acc: 0.7018\n",
      "[Epoch 47/50] Train Loss: 1.4790 | Train Acc: 0.7233 || Val Loss: 1.8772 | Val Acc: 0.7013\n",
      "[Epoch 48/50] Train Loss: 1.4709 | Train Acc: 0.7240 || Val Loss: 1.8790 | Val Acc: 0.7015\n",
      "[Epoch 49/50] Train Loss: 1.4624 | Train Acc: 0.7250 || Val Loss: 1.8847 | Val Acc: 0.7018\n",
      "[Epoch 50/50] Train Loss: 1.4535 | Train Acc: 0.7261 || Val Loss: 1.8940 | Val Acc: 0.7012\n",
      "Training complete. Loading best model weights from checkpoint.\n",
      "Test Full-Sequence Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "###############################################################################\n",
    "# 1. Dataset and Collation\n",
    "###############################################################################\n",
    "class TokenSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset for pre-tokenized sequences stored in memory. \n",
    "    Each sequence is a list of integer token IDs.\n",
    "\n",
    "    This class returns (input_seq, target_seq), where:\n",
    "      - input_seq = sequence[:-1]\n",
    "      - target_seq = sequence[1:]\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences: List[List[int]]):\n",
    "        super().__init__()\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[List[int], List[int]]:\n",
    "        seq = self.sequences[idx]\n",
    "        # We shift to create input/target pairs for next-token prediction\n",
    "        input_seq = seq[:-1]\n",
    "        target_seq = seq[1:]\n",
    "        return input_seq, target_seq\n",
    "\n",
    "\n",
    "class PadCollator:\n",
    "    \"\"\"\n",
    "    A custom collator that pads input sequences to the same length in a batch,\n",
    "    creating attention masks and ensuring alignment of input/target sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, pad_token_id: int = 0):\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __call__(self, batch: List[Tuple[List[int], List[int]]]):\n",
    "        # Extract all input/target pairs\n",
    "        input_batch, target_batch = zip(*batch)\n",
    "\n",
    "        max_len = max(len(seq) for seq in input_batch)\n",
    "\n",
    "        padded_inputs = []\n",
    "        padded_targets = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for inp, tgt in zip(input_batch, target_batch):\n",
    "            inp_len = len(inp)\n",
    "            pad_len = max_len - inp_len\n",
    "\n",
    "            # Pad inputs & targets\n",
    "            padded_inp = inp + [self.pad_token_id] * pad_len\n",
    "            padded_tgt = tgt + [self.pad_token_id] * pad_len\n",
    "\n",
    "            # Create attention mask: 1 for real tokens, 0 for padded\n",
    "            att_mask = [1] * inp_len + [0] * pad_len\n",
    "\n",
    "            padded_inputs.append(padded_inp)\n",
    "            padded_targets.append(padded_tgt)\n",
    "            attention_masks.append(att_mask)\n",
    "\n",
    "        # Convert to tensors\n",
    "        padded_inputs = torch.tensor(padded_inputs, dtype=torch.long)\n",
    "        padded_targets = torch.tensor(padded_targets, dtype=torch.long)\n",
    "        attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "\n",
    "        return padded_inputs, padded_targets, attention_masks\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2. Model Definition\n",
    "###############################################################################\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard sinusoidal positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 50000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x shape: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # Add positional encoding\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A causal (decoder-only) Transformer model for next-token prediction.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 10,\n",
    "        dim_feedforward: int = 1024,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Token Embedding\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "\n",
    "        # Transformer Decoder Layers\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Final projection to vocabulary\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Causal mask cache (for efficiency)\n",
    "        self.register_buffer(\"mask_cache\", None)\n",
    "\n",
    "    def _generate_causal_mask(self, sz: int, device: torch.device):\n",
    "        \"\"\"\n",
    "        Generates an upper-triangular causal mask to ensure each token \n",
    "        can only attend to preceding tokens (including itself).\n",
    "        \"\"\"\n",
    "        if (self.mask_cache is None) or (self.mask_cache.size(0) < sz):\n",
    "            mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1)\n",
    "            # Convert to boolean, True means \"block this position\"\n",
    "            mask = mask.bool()\n",
    "            self.mask_cache = mask\n",
    "        else:\n",
    "            mask = self.mask_cache[:sz, :sz]\n",
    "        return mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_ids: (batch, seq_len)\n",
    "        attention_mask: (batch, seq_len) => 1 for real tokens, 0 for pads\n",
    "\n",
    "        Returns:\n",
    "          logits of shape (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        device = input_ids.device\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        # Generate token embeddings\n",
    "        tok_emb = self.token_emb(input_ids)  # (batch, seq_len, d_model)\n",
    "        # Add positional encoding\n",
    "        pos_emb = self.pos_encoder(tok_emb)\n",
    "\n",
    "        # Prepare the causal mask\n",
    "        causal_mask = self._generate_causal_mask(seq_len, device=device)  # (seq_len, seq_len)\n",
    "\n",
    "        # We also need to expand the attention_mask to shape (batch, 1, seq_len)\n",
    "        # so it can be broadcast to (batch, seq_len, seq_len).\n",
    "        # We'll turn 0 => True in the mask to block those positions.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # So the final mask used by the decoder is (batch, seq_len, seq_len)\n",
    "        # with True where we want to block attention.\n",
    "        combined_mask = causal_mask.unsqueeze(0) | (extended_attention_mask == 0)\n",
    "\n",
    "        # Permute to fit PyTorch's (seq_len, batch, d_model)\n",
    "        pos_emb = pos_emb.permute(1, 0, 2)  # => (seq_len, batch, d_model)\n",
    "\n",
    "        # Decode (TransformerDecoder expects shape (seq_len, batch, d_model))\n",
    "        # The \"memory\" here is empty because we're using a decoder-only approach.\n",
    "        decoded = self.transformer_decoder(\n",
    "            pos_emb,\n",
    "            memory=torch.zeros(0, batch_size, self.d_model, device=device),  # dummy empty memory\n",
    "            tgt_mask=combined_mask[0],  # shape (seq_len, seq_len) for a single batch? We'll do a trick below.\n",
    "            # tgt_key_padding_mask=~attention_mask.bool()  # shape (batch, seq_len)\n",
    "             tgt_key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "\n",
    "        # NOTE: PyTorch's TransformerDecoder can’t directly handle a 3D mask. \n",
    "        # We used 'tgt_key_padding_mask' for pad tokens and 'tgt_mask' for causality. \n",
    "        # This approach merges them. If you have more advanced needs, you'd implement a custom layer or reshape.\n",
    "\n",
    "        # Undo the permute: (seq_len, batch, d_model) -> (batch, seq_len, d_model)\n",
    "        decoded = decoded.permute(1, 0, 2).contiguous()\n",
    "\n",
    "        # Project to vocab\n",
    "        logits = self.output_proj(decoded)  # (batch, seq_len, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. Training / Validation / Testing\n",
    "###############################################################################\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Train for one epoch. Returns (avg_loss, approx_token_accuracy).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets, attention_mask) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs, attention_mask)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "        # Reshape for loss: (batch*seq_len, vocab_size) vs (batch*seq_len)\n",
    "        vocab_size = logits.size(-1)\n",
    "        loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate stats\n",
    "        running_loss += loss.item()\n",
    "        # Approximate token-level accuracy\n",
    "        preds = logits.argmax(dim=-1)  # (batch, seq_len)\n",
    "        mask_flat = attention_mask.view(-1).bool()\n",
    "        correct = (preds.view(-1)[mask_flat] == targets.view(-1)[mask_flat]).sum().item()\n",
    "        count = mask_flat.sum().item()\n",
    "        running_correct += correct\n",
    "        total_tokens += count\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_acc = running_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def validate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Validate the model. Returns (avg_loss, approx_token_accuracy).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, attention_mask in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            logits = model(inputs, attention_mask)\n",
    "            vocab_size = logits.size(-1)\n",
    "            loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Approximate token-level accuracy\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask_flat = attention_mask.view(-1).bool()\n",
    "            correct = (preds.view(-1)[mask_flat] == targets.view(-1)[mask_flat]).sum().item()\n",
    "            count = mask_flat.sum().item()\n",
    "            running_correct += correct\n",
    "            total_tokens += count\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_acc = running_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def test_sequence_accuracy(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the fraction of sequences where the entire predicted sequence\n",
    "    matches the target sequence exactly. \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_sequences = 0\n",
    "    total_sequences = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, attention_mask in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            logits = model(inputs, attention_mask)\n",
    "            preds = logits.argmax(dim=-1)  # (batch, seq_len)\n",
    "\n",
    "            # For each sequence in the batch, compare all tokens (where attention_mask=1).\n",
    "            for i in range(inputs.size(0)):\n",
    "                seq_mask = attention_mask[i].bool()\n",
    "                pred_seq = preds[i, seq_mask]\n",
    "                tgt_seq = targets[i, seq_mask]\n",
    "                total_sequences += 1\n",
    "                if torch.equal(pred_seq, tgt_seq):\n",
    "                    correct_sequences += 1\n",
    "\n",
    "    return correct_sequences / total_sequences if total_sequences > 0 else 0.0\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4. Main Script\n",
    "###############################################################################\n",
    "def main():\n",
    "    # ---------------------------\n",
    "    # 4.1 Load Tokenized Data\n",
    "    # ---------------------------\n",
    "    json_path = \"dataset_encoded.json\"  # Change to your path if needed\n",
    "    with open(json_path, \"r\") as f:\n",
    "        token_data_dict = json.load(f)\n",
    "\n",
    "    # token_data_dict is assumed to be {filename: [list_of_token_ids], ...}\n",
    "    # Merge all token lists into one big list if needed, or keep them separate.\n",
    "    # We'll merge them for a single dataset:\n",
    "    all_sequences = []\n",
    "    for seq_list in token_data_dict.values():\n",
    "        # seq_list is presumably a list of ints\n",
    "        # Possibly it's a list of lists if you segmented each file. \n",
    "        # If needed, adapt to your structure. \n",
    "        # We'll assume it's a single list of token IDs per entry.\n",
    "        if isinstance(seq_list[0], int):\n",
    "            # single sequence\n",
    "            all_sequences.append(seq_list)\n",
    "        else:\n",
    "            # multiple sequences in a sub-list\n",
    "            all_sequences.extend(seq_list)\n",
    "\n",
    "    # Filter out any sequences shorter than 2 tokens (otherwise can't do input/target shift).\n",
    "    all_sequences = [seq for seq in all_sequences if len(seq) > 1]\n",
    "\n",
    "    MAX_CHUNK_LEN = 32  # or any reasonable max length\n",
    "    chunked_sequences = []\n",
    "\n",
    "    for seq_list in token_data_dict.values():\n",
    "        # If seq_list is a single sequence of IDs, chunk it\n",
    "        if isinstance(seq_list[0], int):\n",
    "            seq = seq_list\n",
    "            # Break the single seq into multiple chunks\n",
    "            for i in range(0, len(seq), MAX_CHUNK_LEN):\n",
    "                chunk = seq[i:i+MAX_CHUNK_LEN]\n",
    "                # We only keep chunks that have at least 2 tokens\n",
    "                if len(chunk) > 1:\n",
    "                    chunked_sequences.append(chunk)\n",
    "        else:\n",
    "            # If seq_list is already a list of sequences, chunk each\n",
    "            for seq in seq_list:\n",
    "                for i in range(0, len(seq), MAX_CHUNK_LEN):\n",
    "                    chunk = seq[i:i+MAX_CHUNK_LEN]\n",
    "                    if len(chunk) > 1:\n",
    "                        chunked_sequences.append(chunk)\n",
    "\n",
    "    all_sequences = chunked_sequences\n",
    "\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4.2 Split into Train/Val/Test\n",
    "    # ---------------------------\n",
    "    random.shuffle(all_sequences)\n",
    "    dataset_size = len(all_sequences)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    val_size = int(0.1 * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    train_data, val_data, test_data = random_split(\n",
    "        all_sequences,\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    train_sequences = list(train_data)\n",
    "    val_sequences = list(val_data)\n",
    "    test_sequences = list(test_data)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4.3 Create Datasets & Loaders\n",
    "    # ---------------------------\n",
    "    pad_token_id = 0  # Adjust if your PAD token is something else\n",
    "    train_dataset = TokenSequenceDataset(train_sequences)\n",
    "    val_dataset = TokenSequenceDataset(val_sequences)\n",
    "    test_dataset = TokenSequenceDataset(test_sequences)\n",
    "\n",
    "    collator = PadCollator(pad_token_id=pad_token_id)\n",
    "    batch_size = 256\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collator)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
    "\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(\"Sample input shape:\", sample_batch[0].shape)\n",
    "    print(\"Sample target shape:\", sample_batch[1].shape)\n",
    "    print(\"Sample mask shape:\", sample_batch[2].shape)\n",
    "    print(\"Sample input:\", sample_batch[0][0][:10])  # First 10 tokens of first example\n",
    "    print(\"Sample target:\", sample_batch[1][0][:10])\n",
    "    print(\"Max token ID in input:\", sample_batch[0].max().item())\n",
    "    print(\"Max token ID in target:\", sample_batch[1].max().item())\n",
    "        # ---------------------------\n",
    "    # 4.4 Model, Optimizer, Loss\n",
    "    # ---------------------------\n",
    "    # You likely know your max vocab size from your tokenizer\n",
    "    # For example, if you used Byte-Level BPE or a custom mapping:\n",
    "    vocab_size = 2000  # <-- Replace with your actual vocabulary size\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = DecoderOnlyTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=256,          # Hidden dimension\n",
    "        nhead=8,              # Number of attention heads\n",
    "        num_layers=6,         # Number of transformer decoder layers\n",
    "        dim_feedforward=1024, # FFN dimension\n",
    "        dropout=0.1\n",
    "    )\n",
    "    model.to(device)    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "    pad_token_id=0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4.5 Training Loop\n",
    "    # ---------------------------\n",
    "    epochs = 50\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- Train ---\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        # --- Validate ---\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"[Epoch {epoch}/{epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} || \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Checkpoint if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"transformer_model.pt\")\n",
    "            print(\"  (Best validation loss so far, saving model)\")\n",
    "\n",
    "    print(\"Training complete. Loading best model weights from checkpoint.\")\n",
    "    model.load_state_dict(torch.load(\"transformer_model.pt\"))\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4.6 Evaluate on Test Set (Sequence Accuracy)\n",
    "    # ---------------------------\n",
    "    seq_acc = test_sequence_accuracy(model, test_loader, device)\n",
    "    print(f\"Test Full-Sequence Accuracy: {seq_acc:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harshenv",
   "language": "python",
   "name": "harshenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
